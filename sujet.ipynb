{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet: comment choisir le meilleur modèle d'apprentissage automatique pour un problème donné?"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Déroulement\n",
    "- réalisation en binôme\n",
    "- A rendre : fichier jupyter + éventullement rapport d'analyse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce projet nous allons traiter différents problèmes nécessitant l'utilisation des méthodes d'apprentissage automatique.\n",
    "Le but est de choisir pour un jeu de données, représentant un problème donné, le bon modèle d'apprentissage automatique compatible. Cette tâche englobe notamment le choix du meilleur pré-traitement à appliquer aux jeux de données ainsi que les bons paramètres du modèle d'apprentissage automatique.\n",
    "\n",
    "Ainsi, Résoudre le problème par le clustering revient à fournir une représentation optimale par des clusters du jeu de données associé. En classification supervisée, résoudre le problème signifie produire le modèle prédictif le plus efficace. Nous vous informons qu'un problème dont le jeu de données ne contient pas de labels est résolu globalement par les méthodes de clustering. Si les labels sont présents, dans ce cas, la classification supervisée est souhaitable. Dans un but pédagogique, nous utilisons le même ensemble labélisé de données pour la classification (supervisé) et clustering (ML non supervisé)\n",
    "\n",
    "Pour mener cette tâche, certaines étapes sont nécessaires.\n",
    "\n",
    "- La première étape donne un apperçu sur les jeux de données auxquelles nous nous intéressons. \n",
    "- Les étapes de 2 à 4 demandent à développer des fonctions nécessaires à la préparation des jeux de données. \n",
    "- A partir de l'étape 5, on procède à la recherche du modèle optimal d'apprentissage automatique pour chacun des problèmes."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Dans ce projet nous allons traiter différents problèmes nécessitant l'utilisation des méthodes d'apprentissage automatique.\n",
    "Le but est de choisir pour un jeu de données, représentant un problème donné, le bon modèle d'apprentissage automatique compatible. Cette tâche englobe notamment le choix du meilleur pré-traitement à appliquer aux jeux de données ainsi que les bons paramètres du modèle d'apprentissage automatique.\n",
    "\n",
    "Ainsi, Résoudre le problème par le clustering revient à fournir une représentation optimale par des clusters du jeu de données associé. En classification supervisée, résoudre le problème signifie produire le modèle prédictif le plus efficace. Nous vous informons qu'un problème dont le jeu de données ne contient pas de labels est résolu globalement par les méthodes de clustering. Si les labels sont présents, dans ce cas, la classification supervisée est souhaitable. Dans un but pédagogique, nous utilisons le même ensemble labélisé de données pour la classification (supervisé) et clustering (ML non supervisé)\n",
    "\n",
    "Pour mener cette tâche, certaines étapes sont nécessaires.\n",
    "\n",
    "- La première étape donne un apperçu sur les jeux de données auxquelles nous nous intéressons.\n",
    "- Les étapes de 2 à 4 demandent à développer des fonctions nécessaires à la préparation des jeux de données.\n",
    "- A partir de l'étape 5, on procède à la recherche du modèle optimal d'apprentissage automatique pour chacun des problèmes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <font color = \"darkgreen\">1- Description des jeux de données</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce projet, nous travaillerons sur des problèmes réels que nous cherchons à résoudre au moyen de l'apprentissage automatique :\n",
    "\n",
    "* GCM: diagnostic de cancer multiclasses (prostate, pancréans...) , chaque obervation est une représentation d'un cancer via 16063 signatures d'expression de gènes tumoraux. Le cancer pourrait avoir plusieurs representations et donc plusieurs observations.\n",
    "\n",
    "* Diabetes: diagnostic de diabètes. Une obervation représente les caractéristiques d'un patient (âge, pression, poids...) atteinte ou non du diabète.\n",
    "\n",
    "* Wall-robot-navigation: mouvements du robot SCITOS G5 naviguant dans une pièce en suivant le mur dans le sens des aiguilles d'une montre, pendant 4 tours, mesurés à l'aide de 24 capteurs à ultrasons disposés en cercle autour de sa \"taille\". Chaque obervation représente un mouvement: Avancer,  Léger virage à droite, Virage serré à droite ou Tournez légèrement à gauche (le code peut être fourni )\n",
    "\n",
    "* Japenese-vowel: cet ensemble de données enregistre 640 séries chronologiques de 12 coefficients de cepstre LPC provenant de neuf locuteurs masculins (https://fr.wikipedia.org/wiki/Cepstre). Chaque obervation représente un son d'une voyelle exprimé par un locuteur donné."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = \"darkgreen\">2- Chargement des jeux de données</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous découpons chaque jeu de données en X et y. Pour ce faire, \n",
    "\n",
    "Développez une fonction qui charge le jeu de données depuis un fichier  et retourne deux dataframes (un pour les caractéristiques et l'autre pour la classe/labels)\n",
    "\n",
    "IL vous revient de bien analyser la structure pour determiner quelles sont les caractériqtiques et quels les calsses correspondantes (y). Néaumoins, voici quelques précisions :\n",
    "\n",
    "#### Notes : \n",
    "\n",
    "* Tous les formats sont de type csv\n",
    "* La colonne relative à la classe/label est généralement à position 0 ou à la dernière position\n",
    "* La colonne classe n'a pas le même nom dans les différents jeux de données.\n",
    "* La colonne classe n'est forcément numérique, dans ce cas la conversion s'avère nécessaire. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T19:37:31.502798Z",
     "start_time": "2025-06-30T19:37:31.491814Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "def split_data(path,label):\n",
    "    df = pd.read_csv(path)\n",
    "    X = df.drop(columns=[label] , axis = 1)\n",
    "    y = df[label]\n",
    "\n",
    "    return X, y"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "* Diabetes"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T19:37:31.643795Z",
     "start_time": "2025-06-30T19:37:31.617841Z"
    }
   },
   "cell_type": "code",
   "source": "X_diabetes ,y_diabetes = split_data(\"dataset/data/diabetes.csv\",\"Outcome\")",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "* Japenese-vowel"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T19:37:31.768795Z",
     "start_time": "2025-06-30T19:37:31.709801Z"
    }
   },
   "cell_type": "code",
   "source": "X_japanese ,y_japanese = split_data(\"dataset/data/JapaneseVowels.csv\",\"speaker\")",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " * Wall-robot-navigation"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T19:37:31.940798Z",
     "start_time": "2025-06-30T19:37:31.853800Z"
    }
   },
   "cell_type": "code",
   "source": "X_wrn ,y_wrn = split_data(\"dataset/data/wall-robot-navigation.csv\",\"Class\")",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "* GCM"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T19:37:35.600283Z",
     "start_time": "2025-06-30T19:37:32.019797Z"
    }
   },
   "cell_type": "code",
   "source": "df_gcm = pd.read_csv(\"dataset/data/GCM.csv\" , index_col=1)",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tim\\miniconda3\\envs\\TD\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3072: DtypeWarning: Columns (1,2,4,5,7,8,10,11,12,13,14,16,17,20,21,23,24,26,27,28,29,30,31,32,33,35,36,39,40,42,48,54,55,57,63,64,67,68,69,70,71,73,74,81,83,95,99,101,104,105,108,110,113,114,117,118,122,124,125,133,140,141,144,146,159,161,162,164,167,176,177,178,184,186,187,188,191,192,194,198,204,206,212,213,218,219,224,226,230,235,236,240,241,245,250,255,256,257,259,264,265,266,268,270,271,273,277,278,285,294,302,319,325,329,331,334,340,341,343,348,353,354,366,371,376,382,386,387,388,390,391,393,396,407,408,409,412,413,416,417,418,419,420,421,428,429,438,443,447,454,455,456,459,467,468,470,471,477,484,485,486,490,492,493,503,510,514,516,522,524,545,547,550,556,557,559,561,564,565,569,571,572,573,574,582,585,589,595,596,597,598,600,608,618,620,623,625,626,631,633,636,637,638,643,644,646,650,652,656,657,658,661,664,669,673,674,679,680,683,684,685,687,690,691,692,693,694,696,697,699,703,704,711,713,724,726,727,730,732,734,741,749,751,754,757,760,761,763,768,769,773,776,779,781,784,786,789,791,795,800,802,808,809,810,812,821,822,824,825,826,827,828,829,830,840,842,843,848,849,851,855,856,858,859,862,863,864,865,868,870,871,883,887,888,891,898,901,904,906,908,909,912,919,922,924,926,928,931,935,936,937,938,939,940,942,943,944,945,946,949,950,951,953,956,957,958,961,962,966,969,975,976,977,982,983,984,985,988,991,993,994,998,1000,1003,1004,1005,1006,1011,1013,1015,1016,1017,1018,1019,1022,1026,1028,1029,1030,1032,1036,1037,1038,1040,1042,1044,1047,1050,1052,1058,1066,1068,1074,1078,1079,1086,1088,1090,1091,1092,1094,1097,1098,1099,1101,1102,1105,1118,1121,1122,1123,1128,1134,1135,1137,1138,1141,1142,1143,1147,1150,1159,1161,1163,1164,1166,1167,1171,1174,1181,1182,1183,1184,1185,1189,1190,1192,1198,1203,1205,1206,1207,1209,1211,1212,1213,1214,1217,1218,1220,1230,1235,1236,1237,1239,1240,1242,1244,1246,1249,1251,1258,1260,1261,1262,1268,1270,1271,1272,1274,1275,1280,1282,1283,1284,1289,1291,1292,1301,1304,1305,1307,1310,1312,1313,1322,1325,1335,1336,1337,1338,1339,1340,1343,1347,1350,1366,1373,1378,1384,1394,1395,1400,1401,1408,1411,1412,1414,1415,1418,1422,1424,1425,1430,1434,1437,1438,1439,1442,1443,1445,1448,1449,1453,1456,1457,1460,1464,1470,1471,1473,1474,1476,1484,1485,1488,1491,1492,1493,1494,1502,1503,1504,1505,1506,1508,1511,1514,1515,1516,1519,1521,1523,1524,1525,1526,1528,1531,1532,1536,1542,1547,1552,1553,1554,1555,1557,1558,1559,1563,1565,1569,1570,1573,1577,1581,1582,1584,1588,1589,1590,1592,1593,1594,1596,1599,1600,1601,1603,1605,1609,1614,1617,1625,1628,1629,1631,1634,1639,1645,1646,1647,1654,1655,1662,1664,1666,1670,1675,1677,1678,1679,1681,1686,1687,1688,1689,1690,1691,1696,1698,1701,1705,1708,1711,1715,1717,1718,1721,1722,1725,1726,1734,1735,1737,1739,1742,1744,1747,1748,1750,1751,1753,1754,1756,1757,1767,1768,1771,1776,1778,1780,1785,1786,1791,1793,1794,1796,1798,1804,1805,1806,1810,1812,1814,1815,1816,1818,1821,1823,1832,1837,1839,1842,1843,1844,1845,1847,1849,1851,1852,1857,1861,1862,1864,1865,1868,1872,1873,1874,1878,1879,1880,1883,1886,1887,1888,1893,1895,1897,1901,1905,1906,1910,1912,1914,1922,1924,1925,1926,1933,1935,1936,1937,1938,1939,1942,1945,1946,1947,1949,1950,1955,1957,1960,1963,1965,1967,1972,1975,1984,1988,1996,1999,2000,2005,2006,2010,2013,2016,2017,2021,2026,2027,2029,2033,2035,2036,2038,2039,2043,2044,2045,2046,2049,2053,2054,2056,2058,2066,2069,2070,2071,2073,2077,2079,2081,2085,2088,2089,2090,2091,2096,2098,2100,2106,2112,2113,2116,2117,2118,2125,2126,2128,2130,2131,2132,2136,2139,2141,2143,2149,2151,2153,2154,2155,2157,2159,2160,2162,2166,2169,2173,2174,2175,2177,2179,2181,2188,2190,2191,2192,2193,2197,2199,2200,2201,2202,2203,2205,2206,2207,2208,2211,2212,2217,2220,2223,2224,2225,2230,2231,2232,2233,2237,2241,2244,2245,2249,2253,2254,2257,2260,2261,2265,2266,2270,2273,2275,2278,2279,2280,2282,2284,2290,2291,2294,2295,2296,2300,2301,2306,2308,2311,2312,2313,2320,2321,2322,2326,2327,2333,2334,2335,2336,2340,2341,2345,2346,2350,2351,2352,2353,2355,2356,2357,2358,2362,2365,2372,2375,2377,2378,2381,2384,2386,2387,2388,2398,2399,2401,2402,2406,2411,2414,2415,2420,2422,2424,2429,2430,2433,2434,2439,2443,2445,2446,2454,2465,2467,2469,2470,2471,2472,2475,2480,2482,2485,2487,2489,2491,2494,2496,2499,2501,2504,2516,2517,2519,2522,2523,2525,2526,2529,2530,2531,2534,2538,2539,2542,2544,2546,2551,2552,2554,2555,2557,2560,2565,2570,2572,2573,2576,2577,2583,2584,2587,2588,2592,2606,2609,2612,2614,2625,2628,2629,2631,2632,2633,2636,2637,2639,2643,2645,2649,2651,2652,2659,2662,2668,2675,2678,2680,2682,2684,2686,2690,2691,2694,2698,2699,2703,2708,2711,2714,2716,2717,2720,2723,2725,2730,2731,2732,2735,2737,2740,2746,2750,2762,2766,2771,2772,2774,2776,2779,2780,2786,2788,2799,2805,2810,2811,2813,2814,2816,2820,2822,2825,2826,2830,2831,2833,2834,2835,2836,2837,2839,2840,2843,2849,2851,2853,2855,2856,2857,2858,2859,2868,2869,2870,2871,2873,2877,2880,2882,2883,2890,2895,2896,2898,2901,2907,2908,2910,2916,2921,2922,2923,2925,2927,2930,2932,2936,2937,2939,2940,2941,2943,2953,2954,2956,2961,2962,2966,2967,2968,2970,2971,2972,2975,2976,2979,2982,2983,2985,2991,2996,2999,3000,3006,3008,3011,3012,3014,3015,3018,3020,3023,3024,3025,3027,3034,3035,3036,3038,3042,3043,3045,3050,3052,3056,3057,3064,3066,3069,3070,3072,3073,3077,3079,3082,3084,3085,3087,3090,3091,3092,3093,3094,3097,3099,3100,3101,3102,3103,3114,3115,3117,3118,3119,3120,3124,3130,3132,3133,3135,3139,3141,3142,3143,3144,3150,3152,3154,3155,3157,3158,3159,3163,3165,3166,3172,3178,3180,3185,3186,3187,3188,3189,3191,3192,3194,3196,3200,3201,3202,3209,3210,3212,3213,3215,3216,3217,3219,3223,3224,3237,3244,3249,3261,3263,3270,3273,3275,3277,3279,3286,3287,3288,3290,3292,3297,3298,3299,3300,3302,3303,3304,3305,3316,3318,3323,3326,3329,3331,3334,3339,3342,3343,3351,3361,3362,3367,3372,3379,3380,3381,3382,3383,3385,3387,3388,3394,3399,3400,3406,3407,3411,3413,3415,3416,3417,3418,3419,3422,3424,3426,3433,3436,3437,3441,3442,3446,3454,3456,3457,3458,3465,3467,3481,3483,3485,3487,3489,3490,3493,3495,3498,3500,3501,3511,3512,3513,3515,3517,3518,3521,3523,3524,3525,3526,3527,3533,3534,3535,3536,3539,3540,3542,3543,3546,3549,3550,3552,3554,3555,3562,3567,3571,3581,3586,3594,3600,3601,3605,3607,3608,3609,3612,3614,3615,3619,3620,3621,3622,3623,3624,3631,3634,3635,3639,3647,3650,3652,3655,3659,3660,3661,3664,3669,3671,3673,3676,3677,3678,3679,3680,3682,3683,3686,3687,3688,3690,3692,3694,3695,3696,3702,3703,3708,3710,3711,3714,3718,3720,3722,3723,3724,3730,3731,3732,3734,3735,3736,3744,3745,3746,3748,3750,3752,3755,3767,3770,3774,3775,3778,3782,3783,3784,3790,3791,3792,3793,3795,3796,3798,3800,3801,3805,3808,3809,3810,3813,3815,3816,3818,3819,3825,3826,3828,3831,3836,3837,3838,3839,3840,3842,3844,3848,3849,3853,3859,3862,3863,3864,3870,3872,3875,3878,3881,3886,3889,3890,3894,3895,3898,3899,3901,3904,3910,3912,3916,3917,3918,3919,3920,3922,3923,3926,3931,3933,3937,3941,3944,3946,3950,3951,3952,3954,3955,3956,3959,3965,3966,3968,3970,3972,3975,3977,3981,3982,3988,3993,3995,3996,3998,4001,4006,4010,4012,4015,4021,4022,4023,4028,4031,4032,4033,4046,4047,4048,4053,4055,4056,4057,4062,4065,4066,4068,4069,4070,4072,4073,4075,4076,4080,4082,4084,4085,4087,4088,4090,4097,4098,4099,4100,4101,4102,4104,4105,4106,4108,4109,4113,4120,4121,4122,4125,4126,4130,4133,4134,4136,4143,4153,4154,4156,4157,4163,4164,4168,4169,4171,4179,4188,4189,4190,4191,4192,4196,4197,4200,4203,4205,4207,4208,4211,4212,4213,4215,4216,4218,4219,4220,4222,4223,4226,4228,4229,4230,4232,4237,4240,4248,4250,4251,4252,4254,4256,4257,4258,4260,4262,4264,4265,4268,4275,4276,4278,4280,4281,4282,4286,4289,4292,4294,4295,4299,4306,4310,4312,4315,4319,4320,4322,4323,4326,4329,4330,4332,4334,4337,4338,4346,4348,4354,4355,4357,4359,4363,4371,4378,4379,4384,4386,4400,4402,4403,4406,4407,4408,4409,4413,4414,4416,4417,4419,4420,4425,4426,4427,4431,4434,4442,4444,4448,4449,4450,4454,4455,4456,4457,4461,4463,4465,4475,4477,4478,4499,4501,4502,4507,4508,4511,4514,4517,4521,4522,4528,4529,4536,4537,4540,4547,4560,4561,4567,4568,4569,4571,4573,4575,4577,4578,4583,4586,4590,4594,4595,4596,4602,4603,4606,4608,4609,4617,4625,4626,4627,4628,4630,4631,4633,4639,4644,4660,4661,4666,4670,4671,4674,4678,4680,4683,4684,4686,4690,4692,4700,4701,4703,4707,4708,4710,4712,4713,4715,4717,4718,4719,4720,4722,4724,4725,4726,4732,4733,4734,4735,4736,4742,4743,4744,4747,4749,4750,4751,4754,4755,4756,4760,4764,4765,4772,4775,4776,4777,4781,4788,4792,4794,4797,4798,4803,4805,4806,4811,4812,4813,4814,4815,4817,4819,4821,4824,4826,4827,4829,4831,4832,4833,4838,4842,4843,4844,4845,4847,4857,4862,4868,4870,4871,4872,4875,4876,4877,4878,4882,4883,4884,4885,4889,4890,4892,4894,4897,4901,4902,4904,4906,4908,4909,4910,4913,4914,4915,4919,4923,4925,4926,4929,4932,4933,4937,4939,4944,4953,4954,4956,4957,4959,4962,4965,4966,4968,4969,4970,4971,4973,4977,4984,4985,4989,4994,4995,4998,4999,5000,5001,5002,5003,5004,5005,5010,5012,5014,5015,5016,5019,5021,5023,5024,5028,5031,5035,5036,5037,5038,5039,5046,5048,5050,5052,5053,5055,5059,5061,5063,5067,5068,5069,5070,5071,5072,5075,5077,5083,5084,5097,5098,5099,5100,5103,5107,5109,5113,5114,5117,5118,5123,5126,5127,5128,5132,5134,5135,5140,5143,5154,5156,5157,5159,5161,5165,5167,5170,5171,5172,5177,5184,5187,5188,5192,5194,5195,5197,5199,5202,5203,5204,5205,5208,5209,5210,5211,5212,5214,5215,5217,5218,5220,5221,5222,5224,5225,5227,5229,5232,5235,5240,5246,5248,5250,5256,5257,5260,5266,5268,5275,5278,5279,5285,5294,5304,5312,5315,5321,5325,5326,5333,5334,5347,5353,5355,5357,5359,5367,5370,5371,5372,5376,5377,5378,5379,5380,5386,5388,5391,5392,5393,5394,5395,5397,5398,5403,5407,5410,5412,5416,5418,5422,5423,5424,5425,5428,5429,5431,5432,5433,5434,5436,5437,5445,5448,5452,5453,5455,5456,5458,5459,5460,5465,5470,5475,5480,5481,5489,5490,5491,5493,5495,5498,5499,5502,5503,5507,5508,5510,5511,5512,5514,5515,5523,5525,5526,5527,5533,5537,5545,5546,5549,5551,5552,5560,5563,5567,5569,5571,5575,5576,5577,5581,5584,5585,5589,5591,5594,5595,5596,5597,5598,5599,5600,5601,5602,5606,5609,5610,5612,5614,5615,5621,5622,5626,5627,5634,5637,5639,5655,5658,5668,5669,5671,5674,5679,5680,5681,5684,5685,5690,5691,5694,5695,5697,5698,5699,5700,5705,5708,5710,5711,5720,5721,5723,5730,5731,5733,5734,5735,5736,5742,5743,5744,5746,5748,5751,5752,5753,5756,5757,5758,5759,5763,5764,5765,5766,5768,5769,5770,5771,5773,5775,5776,5783,5784,5787,5789,5790,5791,5800,5803,5807,5808,5810,5812,5813,5826,5827,5830,5832,5838,5840,5853,5855,5856,5859,5862,5865,5866,5867,5868,5869,5875,5876,5879,5880,5888,5891,5893,5894,5896,5899,5901,5903,5905,5908,5911,5912,5914,5915,5918,5919,5924,5926,5928,5931,5933,5935,5940,5941,5943,5945,5951,5952,5953,5957,5959,5961,5962,5964,5965,5969,5973,5976,5980,5987,5997,5998,6001,6002,6003,6005,6007,6010,6011,6015,6016,6020,6023,6026,6027,6028,6030,6033,6037,6043,6044,6045,6047,6048,6051,6053,6055,6061,6062,6064,6067,6068,6069,6070,6076,6077,6081,6082,6083,6084,6089,6090,6092,6096,6097,6101,6103,6108,6110,6114,6117,6120,6124,6126,6127,6128,6134,6141,6144,6145,6146,6147,6148,6149,6150,6151,6154,6159,6162,6164,6167,6169,6173,6174,6176,6181,6182,6184,6186,6188,6190,6192,6193,6194,6197,6198,6199,6201,6202,6203,6209,6210,6213,6215,6216,6218,6219,6220,6227,6228,6230,6231,6234,6236,6239,6240,6244,6245,6251,6253,6254,6255,6256,6257,6262,6263,6265,6266,6271,6273,6275,6278,6281,6285,6286,6288,6290,6291,6293,6295,6298,6301,6302,6305,6311,6312,6315,6316,6322,6329,6332,6335,6338,6342,6343,6344,6346,6348,6350,6353,6354,6355,6359,6360,6362,6363,6367,6369,6370,6371,6384,6385,6387,6389,6392,6396,6399,6403,6404,6405,6407,6408,6409,6411,6416,6419,6420,6422,6429,6431,6435,6436,6445,6448,6451,6453,6455,6458,6459,6460,6468,6469,6470,6478,6479,6480,6481,6482,6484,6486,6487,6491,6496,6497,6498,6503,6504,6507,6509,6512,6517,6518,6521,6522,6523,6529,6530,6532,6535,6537,6540,6541,6542,6543,6544,6545,6549,6551,6553,6559,6562,6564,6565,6569,6571,6576,6579,6583,6586,6587,6590,6591,6593,6598,6600,6602,6604,6605,6613,6616,6621,6627,6628,6629,6630,6631,6632,6633,6634,6636,6639,6641,6643,6645,6647,6649,6650,6651,6652,6654,6656,6657,6661,6662,6663,6666,6667,6672,6678,6679,6682,6683,6686,6689,6690,6693,6694,6695,6696,6697,6701,6704,6705,6706,6707,6709,6711,6713,6714,6715,6722,6725,6731,6734,6735,6736,6739,6741,6742,6746,6747,6749,6751,6752,6754,6755,6756,6757,6758,6761,6762,6767,6769,6771,6774,6775,6776,6777,6781,6782,6783,6784,6786,6791,6797,6798,6800,6803,6808,6809,6817,6818,6819,6823,6831,6832,6833,6839,6840,6841,6842,6844,6848,6850,6852,6859,6861,6863,6864,6865,6867,6869,6870,6874,6875,6878,6881,6882,6885,6886,6887,6890,6891,6892,6898,6906,6908,6912,6916,6920,6921,6922,6923,6930,6932,6934,6937,6939,6940,6945,6955,6957,6959,6960,6961,6962,6963,6964,6967,6969,6973,6977,6979,6980,6982,6985,6989,6990,6992,6993,6994,6995,6997,6999,7004,7006,7007,7010,7011,7016,7020,7021,7024,7027,7028,7030,7032,7033,7034,7038,7043,7045,7046,7048,7050,7051,7052,7053,7059,7066,7067,7073,7074,7077,7079,7081,7082,7083,7085,7087,7089,7091,7092,7094,7099,7104,7105,7107,7109,7111,7113,7119,7121,7123,7124,7125,7126,7128,7130,7131,7134,7135,7136,7139,7140,7142,7143,7145,7146,7148,7149,7150,7151,7152,7154,7155,7160,7161,7163,7164,7166,7167,7168,7169,7170,7171,7172,7173,7175,7176,7179,7180,7182,7188,7194,7195,7197,7201,7203,7210,7211,7212,7215,7217,7221,7222,7232,7234,7239,7240,7242,7245,7246,7250,7251,7255,7256,7258,7259,7267,7269,7272,7276,7279,7283,7293,7294,7296,7298,7300,7301,7304,7307,7310,7312,7313,7314,7316,7320,7326,7332,7333,7343,7346,7350,7357,7358,7361,7363,7365,7367,7368,7369,7370,7372,7375,7376,7380,7381,7382,7386,7388,7392,7393,7394,7397,7399,7403,7404,7406,7409,7414,7416,7420,7425,7428,7430,7431,7433,7434,7439,7441,7445,7448,7451,7452,7453,7457,7458,7460,7462,7464,7465,7468,7470,7472,7478,7479,7480,7484,7486,7487,7488,7491,7493,7494,7497,7499,7508,7511,7513,7519,7520,7523,7524,7525,7532,7540,7542,7543,7544,7547,7550,7551,7554,7555,7560,7567,7574,7576,7577,7578,7581,7583,7585,7586,7587,7591,7593,7596,7597,7598,7601,7603,7605,7607,7609,7611,7613,7617,7618,7621,7629,7630,7631,7632,7635,7636,7638,7640,7648,7649,7651,7653,7655,7656,7657,7660,7661,7662,7669,7670,7672,7676,7678,7689,7690,7691,7695,7700,7706,7712,7713,7719,7723,7727,7728,7729,7733,7734,7736,7737,7739,7741,7747,7748,7753,7756,7758,7759,7760,7761,7762,7766,7767,7768,7770,7771,7772,7773,7774,7776,7777,7778,7779,7780,7781,7782,7783,7784,7786,7788,7790,7792,7796,7798,7800,7803,7804,7807,7808,7809,7813,7814,7815,7818,7821,7828,7832,7837,7841,7846,7850,7851,7854,7855,7856,7863,7866,7869,7874,7880,7882,7883,7884,7886,7887,7895,7896,7904,7910,7913,7917,7927,7930,7931,7933,7937,7938,7940,7941,7942,7944,7945,7946,7950,7951,7957,7963,7965,7969,7970,7972,7973,7979,7984,7988,7992,7995,7999,8002,8006,8009,8014,8015,8021,8022,8023,8025,8029,8031,8032,8034,8039,8041,8044,8045,8048,8049,8052,8055,8056,8057,8064,8065,8066,8067,8069,8070,8073,8078,8080,8082,8083,8085,8087,8089,8092,8098,8100,8102,8103,8110,8112,8113,8114,8118,8120,8124,8126,8130,8131,8133,8136,8137,8138,8142,8143,8147,8153,8157,8162,8167,8169,8171,8172,8173,8175,8178,8179,8181,8186,8195,8201,8202,8205,8206,8207,8211,8216,8222,8224,8225,8226,8227,8228,8229,8230,8231,8233,8234,8239,8240,8241,8244,8245,8251,8252,8255,8256,8258,8259,8261,8263,8265,8267,8269,8270,8271,8274,8275,8276,8278,8279,8280,8282,8283,8287,8289,8292,8295,8296,8298,8299,8302,8304,8307,8308,8311,8314,8315,8316,8319,8322,8327,8328,8329,8330,8333,8339,8340,8341,8342,8343,8353,8354,8355,8356,8357,8360,8361,8362,8363,8365,8366,8367,8368,8372,8373,8374,8379,8380,8381,8383,8387,8389,8392,8393,8396,8397,8401,8405,8406,8412,8414,8416,8419,8421,8422,8427,8430,8436,8440,8442,8444,8446,8448,8449,8450,8452,8453,8456,8459,8460,8463,8466,8469,8472,8473,8474,8478,8482,8485,8490,8499,8504,8505,8506,8507,8510,8515,8516,8518,8519,8520,8525,8527,8528,8529,8530,8533,8539,8540,8541,8542,8543,8545,8551,8554,8557,8558,8560,8563,8565,8567,8568,8573,8574,8577,8578,8581,8583,8585,8588,8591,8595,8608,8609,8610,8611,8612,8616,8617,8620,8621,8622,8623,8624,8628,8635,8636,8637,8638,8640,8642,8643,8646,8647,8649,8650,8652,8655,8656,8659,8660,8662,8664,8666,8673,8681,8683,8684,8686,8692,8696,8697,8701,8702,8703,8704,8707,8708,8709,8710,8713,8715,8716,8721,8727,8728,8734,8735,8741,8747,8750,8754,8755,8756,8759,8765,8768,8770,8776,8778,8784,8785,8793,8794,8795,8796,8799,8803,8806,8808,8810,8811,8812,8814,8815,8816,8822,8825,8828,8829,8830,8831,8832,8833,8834,8835,8836,8841,8844,8846,8849,8850,8851,8852,8854,8855,8861,8865,8868,8869,8872,8873,8875,8881,8882,8883,8885,8886,8888,8894,8895,8899,8900,8901,8904,8906,8907,8908,8914,8915,8916,8917,8918,8921,8923,8926,8927,8928,8929,8932,8933,8934,8935,8936,8938,8941,8946,8951,8956,8957,8964,8965,8966,8967,8971,8972,8976,8983,8987,8989,8990,8991,8994,8995,9003,9004,9007,9014,9015,9019,9021,9022,9026,9027,9028,9029,9034,9041,9048,9050,9051,9054,9055,9057,9058,9060,9062,9064,9066,9068,9070,9072,9074,9075,9085,9088,9096,9097,9098,9099,9102,9103,9107,9110,9112,9114,9115,9117,9118,9129,9135,9139,9142,9144,9147,9148,9149,9152,9154,9156,9159,9160,9161,9162,9167,9170,9177,9180,9181,9183,9184,9186,9188,9189,9193,9195,9198,9199,9201,9202,9204,9210,9213,9216,9219,9220,9224,9228,9229,9232,9236,9241,9250,9253,9254,9255,9270,9278,9284,9285,9292,9297,9300,9309,9314,9319,9320,9321,9322,9324,9330,9331,9335,9337,9338,9340,9342,9343,9344,9347,9348,9349,9350,9351,9355,9357,9361,9362,9363,9364,9365,9368,9369,9371,9372,9374,9378,9386,9391,9392,9394,9398,9400,9404,9405,9409,9411,9424,9425,9427,9434,9436,9437,9438,9439,9440,9442,9446,9449,9450,9451,9455,9456,9457,9458,9461,9462,9464,9468,9469,9470,9474,9475,9476,9477,9479,9480,9481,9483,9485,9488,9489,9490,9491,9492,9500,9503,9505,9507,9513,9516,9517,9522,9528,9529,9530,9531,9532,9534,9536,9537,9540,9542,9544,9548,9550,9554,9557,9562,9563,9566,9567,9568,9570,9575,9579,9582,9583,9585,9603,9604,9605,9606,9607,9610,9615,9618,9620,9621,9623,9625,9637,9638,9649,9652,9656,9658,9659,9663,9666,9672,9673,9674,9675,9677,9679,9681,9682,9684,9685,9687,9688,9689,9691,9695,9697,9699,9701,9704,9706,9708,9709,9712,9713,9714,9715,9716,9718,9719,9721,9722,9724,9725,9727,9728,9746,9747,9748,9749,9751,9752,9757,9758,9759,9762,9766,9769,9775,9781,9789,9791,9794,9796,9800,9804,9806,9807,9808,9809,9810,9813,9814,9816,9818,9819,9820,9821,9823,9824,9826,9829,9833,9834,9835,9836,9837,9838,9839,9840,9841,9843,9846,9849,9851,9856,9857,9861,9862,9870,9873,9874,9875,9877,9878,9881,9885,9888,9889,9890,9891,9896,9898,9900,9901,9902,9904,9905,9908,9909,9910,9912,9913,9914,9915,9916,9928,9930,9931,9933,9934,9935,9937,9938,9940,9945,9947,9948,9949,9950,9951,9952,9954,9957,9958,9960,9968,9970,9973,9974,9975,9976,9977,9979,9980,9981,9988,9991,9993,9996,10001,10002,10005,10007,10010,10011,10016,10017,10018,10019,10020,10022,10028,10030,10031,10036,10040,10041,10043,10044,10045,10050,10057,10058,10060,10061,10064,10068,10071,10072,10074,10076,10078,10085,10087,10095,10097,10099,10100,10106,10108,10115,10117,10118,10119,10123,10124,10125,10128,10129,10130,10131,10135,10137,10140,10143,10144,10145,10148,10150,10151,10154,10156,10161,10163,10168,10170,10172,10177,10184,10193,10194,10202,10204,10208,10212,10214,10221,10223,10224,10225,10227,10230,10231,10232,10233,10239,10245,10246,10248,10251,10262,10264,10267,10270,10273,10280,10281,10282,10284,10286,10287,10288,10290,10291,10295,10296,10303,10304,10306,10307,10309,10310,10313,10319,10320,10322,10323,10325,10326,10327,10328,10330,10332,10333,10336,10338,10339,10340,10342,10348,10349,10351,10352,10353,10360,10361,10363,10364,10365,10369,10370,10371,10376,10380,10381,10384,10385,10387,10388,10395,10397,10398,10400,10404,10406,10407,10410,10415,10416,10417,10419,10420,10422,10424,10426,10427,10428,10429,10430,10435,10440,10442,10447,10452,10453,10456,10465,10467,10470,10472,10474,10475,10477,10482,10484,10485,10490,10493,10494,10495,10500,10506,10507,10510,10511,10514,10516,10517,10518,10521,10526,10527,10529,10531,10532,10533,10535,10537,10538,10540,10541,10544,10545,10546,10547,10550,10551,10554,10555,10557,10564,10565,10566,10567,10568,10576,10579,10580,10581,10582,10583,10585,10589,10594,10596,10605,10612,10613,10614,10618,10622,10623,10625,10626,10627,10628,10631,10636,10637,10638,10639,10641,10643,10646,10649,10650,10657,10658,10661,10664,10665,10669,10671,10672,10673,10674,10675,10676,10677,10678,10679,10684,10685,10686,10692,10694,10701,10703,10705,10706,10707,10710,10713,10718,10719,10721,10726,10729,10730,10732,10734,10738,10749,10750,10751,10754,10757,10758,10761,10765,10767,10769,10772,10774,10777,10781,10782,10783,10784,10789,10791,10794,10795,10796,10797,10798,10799,10802,10807,10809,10810,10811,10816,10817,10820,10822,10825,10827,10831,10833,10836,10837,10841,10843,10845,10847,10852,10854,10856,10858,10860,10863,10864,10865,10868,10871,10872,10876,10877,10881,10885,10886,10887,10889,10890,10893,10898,10900,10901,10902,10903,10906,10911,10912,10913,10916,10919,10921,10922,10924,10926,10927,10928,10929,10939,10940,10941,10943,10944,10946,10956,10959,10960,10962,10963,10965,10969,10971,10972,10974,10975,10979,10980,10981,10984,10986,10989,10991,10992,10994,10997,11011,11014,11018,11019,11020,11021,11025,11027,11034,11035,11038,11040,11042,11044,11045,11046,11049,11056,11057,11058,11059,11062,11063,11066,11068,11069,11071,11073,11076,11077,11078,11079,11083,11086,11088,11089,11090,11093,11094,11095,11099,11100,11103,11107,11108,11109,11110,11112,11113,11114,11119,11120,11121,11124,11129,11131,11132,11133,11134,11135,11137,11138,11141,11143,11145,11152,11153,11154,11156,11165,11170,11171,11174,11175,11176,11177,11179,11181,11182,11184,11187,11189,11191,11192,11195,11198,11199,11200,11203,11206,11208,11209,11211,11216,11217,11218,11222,11223,11224,11228,11230,11233,11236,11239,11240,11241,11242,11244,11246,11250,11251,11253,11256,11261,11268,11269,11273,11280,11287,11292,11295,11300,11301,11315,11319,11326,11327,11328,11333,11334,11335,11336,11338,11341,11343,11349,11352,11356,11358,11360,11361,11367,11378,11379,11381,11382,11387,11389,11390,11391,11392,11393,11396,11401,11402,11404,11406,11408,11412,11417,11418,11419,11421,11422,11430,11433,11434,11435,11436,11439,11440,11441,11442,11445,11447,11448,11449,11450,11451,11454,11457,11462,11465,11468,11470,11473,11476,11477,11484,11488,11492,11495,11496,11498,11500,11503,11511,11513,11515,11518,11519,11521,11522,11524,11525,11526,11528,11530,11532,11534,11535,11536,11537,11538,11539,11540,11541,11545,11546,11549,11550,11551,11552,11553,11559,11565,11569,11572,11575,11576,11579,11580,11581,11583,11585,11586,11589,11592,11597,11599,11600,11605,11610,11613,11614,11615,11616,11617,11619,11623,11626,11629,11633,11634,11635,11642,11645,11648,11654,11656,11658,11662,11664,11666,11674,11681,11683,11691,11694,11695,11697,11703,11704,11706,11711,11716,11718,11720,11722,11729,11730,11733,11736,11737,11739,11740,11743,11744,11748,11750,11754,11758,11760,11761,11762,11763,11765,11766,11767,11769,11772,11775,11782,11786,11791,11795,11804,11805,11807,11808,11810,11811,11814,11818,11819,11821,11824,11831,11833,11834,11836,11837,11842,11848,11850,11854,11856,11857,11860,11865,11873,11874,11880,11881,11887,11894,11895,11897,11898,11899,11901,11908,11909,11912,11913,11915,11916,11917,11918,11921,11928,11929,11930,11932,11938,11941,11942,11945,11946,11947,11952,11954,11955,11956,11964,11966,11976,11980,11981,11982,11983,11993,11996,11997,11998,11999,12002,12006,12009,12012,12016,12018,12024,12025,12026,12027,12032,12033,12036,12039,12042,12044,12045,12050,12053,12054,12056,12059,12060,12071,12072,12073,12074,12080,12084,12086,12091,12093,12098,12103,12106,12123,12127,12130,12132,12141,12148,12157,12159,12162,12163,12166,12167,12169,12173,12175,12176,12178,12181,12189,12190,12191,12194,12198,12208,12209,12210,12212,12219,12221,12222,12226,12228,12231,12232,12241,12244,12246,12251,12253,12256,12257,12258,12260,12262,12263,12264,12265,12266,12267,12274,12276,12281,12282,12284,12288,12289,12290,12292,12293,12294,12296,12298,12301,12308,12313,12317,12318,12319,12326,12332,12333,12335,12338,12340,12344,12346,12347,12351,12358,12360,12362,12367,12369,12371,12377,12385,12389,12390,12392,12395,12398,12401,12403,12406,12408,12409,12412,12418,12419,12421,12422,12423,12425,12426,12427,12428,12429,12430,12435,12436,12437,12440,12441,12443,12445,12446,12452,12456,12458,12460,12469,12471,12472,12473,12475,12476,12480,12492,12493,12496,12498,12499,12501,12502,12504,12505,12506,12509,12515,12518,12519,12527,12531,12534,12538,12543,12545,12546,12555,12558,12560,12564,12567,12570,12571,12573,12574,12577,12580,12581,12584,12586,12587,12588,12591,12595,12598,12600,12603,12604,12607,12609,12610,12613,12614,12624,12625,12629,12631,12632,12636,12637,12638,12640,12642,12646,12650,12651,12663,12667,12668,12669,12672,12676,12677,12682,12686,12687,12688,12690,12692,12693,12696,12702,12710,12712,12714,12718,12721,12723,12726,12728,12729,12730,12731,12732,12734,12745,12756,12758,12760,12763,12766,12770,12777,12786,12787,12799,12800,12809,12811,12823,12833,12834,12835,12837,12840,12849,12851,12853,12854,12855,12856,12866,12869,12871,12880,12891,12892,12894,12901,12902,12903,12909,12911,12913,12916,12918,12919,12920,12921,12923,12924,12927,12931,12934,12939,12941,12943,12947,12948,12950,12952,12958,12962,12963,12967,12968,12970,12976,12991,12995,12996,12998,12999,13000,13002,13008,13009,13012,13015,13018,13019,13021,13022,13025,13027,13028,13030,13032,13039,13044,13051,13055,13056,13057,13060,13067,13068,13069,13073,13085,13089,13090,13094,13100,13107,13108,13111,13113,13115,13117,13119,13122,13126,13130,13134,13140,13145,13146,13152,13157,13161,13162,13168,13170,13173,13174,13177,13178,13180,13181,13189,13190,13195,13197,13206,13207,13212,13218,13219,13220,13221,13237,13238,13239,13250,13252,13253,13256,13260,13267,13273,13275,13277,13279,13280,13282,13285,13286,13289,13294,13296,13301,13305,13308,13309,13314,13318,13326,13332,13333,13337,13344,13347,13353,13356,13357,13360,13361,13365,13366,13369,13376,13377,13381,13383,13385,13389,13390,13393,13394,13396,13399,13400,13409,13410,13411,13417,13422,13423,13425,13431,13433,13435,13437,13439,13441,13446,13449,13466,13467,13474,13476,13479,13481,13482,13491,13492,13496,13497,13502,13508,13509,13511,13512,13513,13515,13517,13519,13522,13524,13527,13528,13529,13532,13533,13536,13537,13538,13541,13543,13544,13549,13553,13554,13556,13557,13570,13571,13573,13574,13587,13591,13598,13599,13603,13618,13628,13629,13630,13637,13640,13645,13651,13654,13656,13674,13675,13678,13679,13681,13685,13686,13689,13690,13692,13698,13704,13708,13709,13710,13714,13723,13729,13731,13732,13734,13735,13737,13738,13740,13742,13744,13745,13746,13749,13750,13754,13760,13762,13764,13768,13772,13779,13781,13783,13785,13787,13789,13792,13793,13797,13798,13803,13810,13819,13820,13821,13824,13828,13829,13833,13836,13840,13841,13842,13843,13846,13847,13848,13854,13855,13859,13863,13865,13869,13872,13877,13881,13884,13888,13892,13896,13901,13902,13907,13908,13911,13914,13917,13918,13922,13923,13924,13926,13927,13928,13929,13931,13939,13940,13944,13946,13948,13952,13954,13956,13960,13962,13967,13968,13976,13979,13992,13993,13994,13995,14001,14005,14007,14008,14018,14019,14025,14027,14028,14030,14034,14038,14046,14048,14050,14051,14052,14055,14056,14059,14067,14072,14074,14076,14077,14083,14092,14102,14103,14105,14106,14107,14109,14110,14115,14119,14123,14125,14126,14128,14129,14130,14136,14142,14149,14151,14155,14156,14158,14160,14169,14170,14175,14177,14180,14183,14185,14190,14193,14199,14203,14207,14213,14214,14221,14223,14224,14226,14227,14232,14233,14234,14237,14238,14239,14241,14244,14248,14249,14258,14264,14272,14274,14275,14280,14284,14289,14296,14300,14304,14305,14310,14312,14317,14322,14327,14329,14334,14340,14341,14345,14351,14358,14359,14360,14368,14371,14374,14376,14384,14387,14388,14395,14397,14399,14403,14404,14412,14413,14419,14420,14423,14424,14428,14429,14431,14432,14433,14436,14438,14439,14440,14445,14455,14462,14464,14467,14470,14471,14473,14480,14482,14483,14488,14492,14495,14502,14506,14508,14511,14512,14519,14521,14525,14526,14527,14528,14530,14534,14536,14538,14539,14540,14541,14543,14545,14550,14554,14558,14562,14564,14567,14571,14572,14573,14576,14583,14587,14593,14600,14601,14602,14608,14611,14614,14615,14618,14619,14620,14624,14628,14635,14642,14647,14651,14654,14659,14673,14674,14675,14681,14682,14690,14692,14701,14703,14707,14708,14710,14725,14726,14727,14728,14739,14741,14758,14768,14769,14773,14778,14779,14780,14783,14786,14787,14789,14791,14792,14793,14795,14796,14798,14799,14806,14807,14809,14813,14814,14818,14820,14821,14825,14832,14835,14840,14842,14843,14846,14848,14849,14851,14853,14854,14855,14856,14857,14859,14860,14861,14862,14864,14866,14870,14878,14879,14883,14885,14886,14888,14890,14891,14894,14895,14904,14905,14911,14914,14921,14924,14928,14931,14933,14936,14937,14938,14941,14944,14948,14954,14971,14973,14975,14976,14978,14986,14987,14995,15000,15001,15003,15005,15006,15008,15010,15019,15021,15022,15024,15025,15026,15027,15030,15035,15041,15042,15043,15044,15046,15055,15061,15069,15070,15072,15078,15080,15082,15088,15090,15096,15097,15101,15107,15108,15111,15113,15116,15117,15121,15123,15125,15143,15145,15149,15152,15154,15156,15158,15159,15160,15162,15169,15176,15177,15178,15189,15193,15194,15196,15197,15199,15208,15209,15217,15220,15229,15234,15236,15240,15244,15246,15247,15251,15252,15260,15261,15262,15267,15268,15269,15270,15272,15277,15279,15290,15291,15292,15293,15294,15301,15304,15305,15307,15311,15315,15318,15322,15323,15324,15328,15331,15338,15342,15354,15361,15362,15364,15367,15374,15381,15383,15387,15394,15397,15400,15402,15404,15406,15409,15411,15424,15429,15432,15440,15442,15448,15449,15450,15453,15460,15462,15465,15473,15484,15485,15487,15495,15499,15500,15501,15503,15504,15505,15508,15509,15511,15513,15520,15527,15530,15542,15546,15547,15552,15554,15563,15568,15570,15571,15576,15579,15581,15582,15584,15586,15590,15591,15595,15597,15598,15600,15601,15604,15607,15609,15614,15624,15631,15632,15634,15641,15643,15648,15649,15650,15651,15653,15654,15659,15661,15668,15670,15671,15674,15675,15677,15681,15690,15692,15693,15695,15702,15708,15718,15722,15723,15724,15725,15727,15729,15734,15736,15741,15743,15745,15751,15753,15754,15755,15759,15762,15767,15768,15772,15775,15777,15778,15779,15780,15784,15786,15788,15789,15791,15792,15794,15795,15798,15800,15803,15806,15807,15817,15818,15819,15823,15824,15829,15832,15840,15842,15847,15852,15854,15856,15858,15861,15865,15867,15872,15873,15874,15875,15876,15880,15881,15889,15891,15892,15893,15895,15899,15904,15910,15911,15913,15914,15915,15917,15920,15922,15923,15924,15927,15928,15929,15930,15932,15933,15935,15937,15938,15939,15945,15946,15949,15951,15952,15953,15955,15956,15957,15961,15963,15969,15979,15981,15982,15984,15985,15989,15990,15996,16001,16002,16004,16011,16012,16013,16014,16015,16016,16017,16019,16020,16023,16026,16027,16028,16031,16032,16033,16034,16035,16036,16037,16038,16039,16043,16044,16051,16052,16053,16057,16059,16060,16062) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T19:37:35.707282Z",
     "start_time": "2025-06-30T19:37:35.679284Z"
    }
   },
   "cell_type": "code",
   "source": "df_gcm.shape\n",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(190, 16063)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T19:37:39.572444Z",
     "start_time": "2025-06-30T19:37:35.791286Z"
    }
   },
   "cell_type": "code",
   "source": "X_gcm , y_gcm = split_data(\"dataset/data/GCM.csv\", \"class\")",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tim\\miniconda3\\envs\\TD\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3263: DtypeWarning: Columns (1,2,4,5,7,8,10,11,12,13,14,16,17,20,21,23,24,26,27,28,29,30,31,32,33,35,36,39,40,42,48,54,55,57,63,64,67,68,69,70,71,73,74,81,83,95,99,101,104,105,108,110,113,114,117,118,122,124,125,133,140,141,144,146,159,161,162,164,167,176,177,178,184,186,187,188,191,192,194,198,204,206,212,213,218,219,224,226,230,235,236,240,241,245,250,255,256,257,259,264,265,266,268,270,271,273,277,278,285,294,302,319,325,329,331,334,340,341,343,348,353,354,366,371,376,382,386,387,388,390,391,393,396,407,408,409,412,413,416,417,418,419,420,421,428,429,438,443,447,454,455,456,459,467,468,470,471,477,484,485,486,490,492,493,503,510,514,516,522,524,545,547,550,556,557,559,561,564,565,569,571,572,573,574,582,585,589,595,596,597,598,600,608,618,620,623,625,626,631,633,636,637,638,643,644,646,650,652,656,657,658,661,664,669,673,674,679,680,683,684,685,687,690,691,692,693,694,696,697,699,703,704,711,713,724,726,727,730,732,734,741,749,751,754,757,760,761,763,768,769,773,776,779,781,784,786,789,791,795,800,802,808,809,810,812,821,822,824,825,826,827,828,829,830,840,842,843,848,849,851,855,856,858,859,862,863,864,865,868,870,871,883,887,888,891,898,901,904,906,908,909,912,919,922,924,926,928,931,935,936,937,938,939,940,942,943,944,945,946,949,950,951,953,956,957,958,961,962,966,969,975,976,977,982,983,984,985,988,991,993,994,998,1000,1003,1004,1005,1006,1011,1013,1015,1016,1017,1018,1019,1022,1026,1028,1029,1030,1032,1036,1037,1038,1040,1042,1044,1047,1050,1052,1058,1066,1068,1074,1078,1079,1086,1088,1090,1091,1092,1094,1097,1098,1099,1101,1102,1105,1118,1121,1122,1123,1128,1134,1135,1137,1138,1141,1142,1143,1147,1150,1159,1161,1163,1164,1166,1167,1171,1174,1181,1182,1183,1184,1185,1189,1190,1192,1198,1203,1205,1206,1207,1209,1211,1212,1213,1214,1217,1218,1220,1230,1235,1236,1237,1239,1240,1242,1244,1246,1249,1251,1258,1260,1261,1262,1268,1270,1271,1272,1274,1275,1280,1282,1283,1284,1289,1291,1292,1301,1304,1305,1307,1310,1312,1313,1322,1325,1335,1336,1337,1338,1339,1340,1343,1347,1350,1366,1373,1378,1384,1394,1395,1400,1401,1408,1411,1412,1414,1415,1418,1422,1424,1425,1430,1434,1437,1438,1439,1442,1443,1445,1448,1449,1453,1456,1457,1460,1464,1470,1471,1473,1474,1476,1484,1485,1488,1491,1492,1493,1494,1502,1503,1504,1505,1506,1508,1511,1514,1515,1516,1519,1521,1523,1524,1525,1526,1528,1531,1532,1536,1542,1547,1552,1553,1554,1555,1557,1558,1559,1563,1565,1569,1570,1573,1577,1581,1582,1584,1588,1589,1590,1592,1593,1594,1596,1599,1600,1601,1603,1605,1609,1614,1617,1625,1628,1629,1631,1634,1639,1645,1646,1647,1654,1655,1662,1664,1666,1670,1675,1677,1678,1679,1681,1686,1687,1688,1689,1690,1691,1696,1698,1701,1705,1708,1711,1715,1717,1718,1721,1722,1725,1726,1734,1735,1737,1739,1742,1744,1747,1748,1750,1751,1753,1754,1756,1757,1767,1768,1771,1776,1778,1780,1785,1786,1791,1793,1794,1796,1798,1804,1805,1806,1810,1812,1814,1815,1816,1818,1821,1823,1832,1837,1839,1842,1843,1844,1845,1847,1849,1851,1852,1857,1861,1862,1864,1865,1868,1872,1873,1874,1878,1879,1880,1883,1886,1887,1888,1893,1895,1897,1901,1905,1906,1910,1912,1914,1922,1924,1925,1926,1933,1935,1936,1937,1938,1939,1942,1945,1946,1947,1949,1950,1955,1957,1960,1963,1965,1967,1972,1975,1984,1988,1996,1999,2000,2005,2006,2010,2013,2016,2017,2021,2026,2027,2029,2033,2035,2036,2038,2039,2043,2044,2045,2046,2049,2053,2054,2056,2058,2066,2069,2070,2071,2073,2077,2079,2081,2085,2088,2089,2090,2091,2096,2098,2100,2106,2112,2113,2116,2117,2118,2125,2126,2128,2130,2131,2132,2136,2139,2141,2143,2149,2151,2153,2154,2155,2157,2159,2160,2162,2166,2169,2173,2174,2175,2177,2179,2181,2188,2190,2191,2192,2193,2197,2199,2200,2201,2202,2203,2205,2206,2207,2208,2211,2212,2217,2220,2223,2224,2225,2230,2231,2232,2233,2237,2241,2244,2245,2249,2253,2254,2257,2260,2261,2265,2266,2270,2273,2275,2278,2279,2280,2282,2284,2290,2291,2294,2295,2296,2300,2301,2306,2308,2311,2312,2313,2320,2321,2322,2326,2327,2333,2334,2335,2336,2340,2341,2345,2346,2350,2351,2352,2353,2355,2356,2357,2358,2362,2365,2372,2375,2377,2378,2381,2384,2386,2387,2388,2398,2399,2401,2402,2406,2411,2414,2415,2420,2422,2424,2429,2430,2433,2434,2439,2443,2445,2446,2454,2465,2467,2469,2470,2471,2472,2475,2480,2482,2485,2487,2489,2491,2494,2496,2499,2501,2504,2516,2517,2519,2522,2523,2525,2526,2529,2530,2531,2534,2538,2539,2542,2544,2546,2551,2552,2554,2555,2557,2560,2565,2570,2572,2573,2576,2577,2583,2584,2587,2588,2592,2606,2609,2612,2614,2625,2628,2629,2631,2632,2633,2636,2637,2639,2643,2645,2649,2651,2652,2659,2662,2668,2675,2678,2680,2682,2684,2686,2690,2691,2694,2698,2699,2703,2708,2711,2714,2716,2717,2720,2723,2725,2730,2731,2732,2735,2737,2740,2746,2750,2762,2766,2771,2772,2774,2776,2779,2780,2786,2788,2799,2805,2810,2811,2813,2814,2816,2820,2822,2825,2826,2830,2831,2833,2834,2835,2836,2837,2839,2840,2843,2849,2851,2853,2855,2856,2857,2858,2859,2868,2869,2870,2871,2873,2877,2880,2882,2883,2890,2895,2896,2898,2901,2907,2908,2910,2916,2921,2922,2923,2925,2927,2930,2932,2936,2937,2939,2940,2941,2943,2953,2954,2956,2961,2962,2966,2967,2968,2970,2971,2972,2975,2976,2979,2982,2983,2985,2991,2996,2999,3000,3006,3008,3011,3012,3014,3015,3018,3020,3023,3024,3025,3027,3034,3035,3036,3038,3042,3043,3045,3050,3052,3056,3057,3064,3066,3069,3070,3072,3073,3077,3079,3082,3084,3085,3087,3090,3091,3092,3093,3094,3097,3099,3100,3101,3102,3103,3114,3115,3117,3118,3119,3120,3124,3130,3132,3133,3135,3139,3141,3142,3143,3144,3150,3152,3154,3155,3157,3158,3159,3163,3165,3166,3172,3178,3180,3185,3186,3187,3188,3189,3191,3192,3194,3196,3200,3201,3202,3209,3210,3212,3213,3215,3216,3217,3219,3223,3224,3237,3244,3249,3261,3263,3270,3273,3275,3277,3279,3286,3287,3288,3290,3292,3297,3298,3299,3300,3302,3303,3304,3305,3316,3318,3323,3326,3329,3331,3334,3339,3342,3343,3351,3361,3362,3367,3372,3379,3380,3381,3382,3383,3385,3387,3388,3394,3399,3400,3406,3407,3411,3413,3415,3416,3417,3418,3419,3422,3424,3426,3433,3436,3437,3441,3442,3446,3454,3456,3457,3458,3465,3467,3481,3483,3485,3487,3489,3490,3493,3495,3498,3500,3501,3511,3512,3513,3515,3517,3518,3521,3523,3524,3525,3526,3527,3533,3534,3535,3536,3539,3540,3542,3543,3546,3549,3550,3552,3554,3555,3562,3567,3571,3581,3586,3594,3600,3601,3605,3607,3608,3609,3612,3614,3615,3619,3620,3621,3622,3623,3624,3631,3634,3635,3639,3647,3650,3652,3655,3659,3660,3661,3664,3669,3671,3673,3676,3677,3678,3679,3680,3682,3683,3686,3687,3688,3690,3692,3694,3695,3696,3702,3703,3708,3710,3711,3714,3718,3720,3722,3723,3724,3730,3731,3732,3734,3735,3736,3744,3745,3746,3748,3750,3752,3755,3767,3770,3774,3775,3778,3782,3783,3784,3790,3791,3792,3793,3795,3796,3798,3800,3801,3805,3808,3809,3810,3813,3815,3816,3818,3819,3825,3826,3828,3831,3836,3837,3838,3839,3840,3842,3844,3848,3849,3853,3859,3862,3863,3864,3870,3872,3875,3878,3881,3886,3889,3890,3894,3895,3898,3899,3901,3904,3910,3912,3916,3917,3918,3919,3920,3922,3923,3926,3931,3933,3937,3941,3944,3946,3950,3951,3952,3954,3955,3956,3959,3965,3966,3968,3970,3972,3975,3977,3981,3982,3988,3993,3995,3996,3998,4001,4006,4010,4012,4015,4021,4022,4023,4028,4031,4032,4033,4046,4047,4048,4053,4055,4056,4057,4062,4065,4066,4068,4069,4070,4072,4073,4075,4076,4080,4082,4084,4085,4087,4088,4090,4097,4098,4099,4100,4101,4102,4104,4105,4106,4108,4109,4113,4120,4121,4122,4125,4126,4130,4133,4134,4136,4143,4153,4154,4156,4157,4163,4164,4168,4169,4171,4179,4188,4189,4190,4191,4192,4196,4197,4200,4203,4205,4207,4208,4211,4212,4213,4215,4216,4218,4219,4220,4222,4223,4226,4228,4229,4230,4232,4237,4240,4248,4250,4251,4252,4254,4256,4257,4258,4260,4262,4264,4265,4268,4275,4276,4278,4280,4281,4282,4286,4289,4292,4294,4295,4299,4306,4310,4312,4315,4319,4320,4322,4323,4326,4329,4330,4332,4334,4337,4338,4346,4348,4354,4355,4357,4359,4363,4371,4378,4379,4384,4386,4400,4402,4403,4406,4407,4408,4409,4413,4414,4416,4417,4419,4420,4425,4426,4427,4431,4434,4442,4444,4448,4449,4450,4454,4455,4456,4457,4461,4463,4465,4475,4477,4478,4499,4501,4502,4507,4508,4511,4514,4517,4521,4522,4528,4529,4536,4537,4540,4547,4560,4561,4567,4568,4569,4571,4573,4575,4577,4578,4583,4586,4590,4594,4595,4596,4602,4603,4606,4608,4609,4617,4625,4626,4627,4628,4630,4631,4633,4639,4644,4660,4661,4666,4670,4671,4674,4678,4680,4683,4684,4686,4690,4692,4700,4701,4703,4707,4708,4710,4712,4713,4715,4717,4718,4719,4720,4722,4724,4725,4726,4732,4733,4734,4735,4736,4742,4743,4744,4747,4749,4750,4751,4754,4755,4756,4760,4764,4765,4772,4775,4776,4777,4781,4788,4792,4794,4797,4798,4803,4805,4806,4811,4812,4813,4814,4815,4817,4819,4821,4824,4826,4827,4829,4831,4832,4833,4838,4842,4843,4844,4845,4847,4857,4862,4868,4870,4871,4872,4875,4876,4877,4878,4882,4883,4884,4885,4889,4890,4892,4894,4897,4901,4902,4904,4906,4908,4909,4910,4913,4914,4915,4919,4923,4925,4926,4929,4932,4933,4937,4939,4944,4953,4954,4956,4957,4959,4962,4965,4966,4968,4969,4970,4971,4973,4977,4984,4985,4989,4994,4995,4998,4999,5000,5001,5002,5003,5004,5005,5010,5012,5014,5015,5016,5019,5021,5023,5024,5028,5031,5035,5036,5037,5038,5039,5046,5048,5050,5052,5053,5055,5059,5061,5063,5067,5068,5069,5070,5071,5072,5075,5077,5083,5084,5097,5098,5099,5100,5103,5107,5109,5113,5114,5117,5118,5123,5126,5127,5128,5132,5134,5135,5140,5143,5154,5156,5157,5159,5161,5165,5167,5170,5171,5172,5177,5184,5187,5188,5192,5194,5195,5197,5199,5202,5203,5204,5205,5208,5209,5210,5211,5212,5214,5215,5217,5218,5220,5221,5222,5224,5225,5227,5229,5232,5235,5240,5246,5248,5250,5256,5257,5260,5266,5268,5275,5278,5279,5285,5294,5304,5312,5315,5321,5325,5326,5333,5334,5347,5353,5355,5357,5359,5367,5370,5371,5372,5376,5377,5378,5379,5380,5386,5388,5391,5392,5393,5394,5395,5397,5398,5403,5407,5410,5412,5416,5418,5422,5423,5424,5425,5428,5429,5431,5432,5433,5434,5436,5437,5445,5448,5452,5453,5455,5456,5458,5459,5460,5465,5470,5475,5480,5481,5489,5490,5491,5493,5495,5498,5499,5502,5503,5507,5508,5510,5511,5512,5514,5515,5523,5525,5526,5527,5533,5537,5545,5546,5549,5551,5552,5560,5563,5567,5569,5571,5575,5576,5577,5581,5584,5585,5589,5591,5594,5595,5596,5597,5598,5599,5600,5601,5602,5606,5609,5610,5612,5614,5615,5621,5622,5626,5627,5634,5637,5639,5655,5658,5668,5669,5671,5674,5679,5680,5681,5684,5685,5690,5691,5694,5695,5697,5698,5699,5700,5705,5708,5710,5711,5720,5721,5723,5730,5731,5733,5734,5735,5736,5742,5743,5744,5746,5748,5751,5752,5753,5756,5757,5758,5759,5763,5764,5765,5766,5768,5769,5770,5771,5773,5775,5776,5783,5784,5787,5789,5790,5791,5800,5803,5807,5808,5810,5812,5813,5826,5827,5830,5832,5838,5840,5853,5855,5856,5859,5862,5865,5866,5867,5868,5869,5875,5876,5879,5880,5888,5891,5893,5894,5896,5899,5901,5903,5905,5908,5911,5912,5914,5915,5918,5919,5924,5926,5928,5931,5933,5935,5940,5941,5943,5945,5951,5952,5953,5957,5959,5961,5962,5964,5965,5969,5973,5976,5980,5987,5997,5998,6001,6002,6003,6005,6007,6010,6011,6015,6016,6020,6023,6026,6027,6028,6030,6033,6037,6043,6044,6045,6047,6048,6051,6053,6055,6061,6062,6064,6067,6068,6069,6070,6076,6077,6081,6082,6083,6084,6089,6090,6092,6096,6097,6101,6103,6108,6110,6114,6117,6120,6124,6126,6127,6128,6134,6141,6144,6145,6146,6147,6148,6149,6150,6151,6154,6159,6162,6164,6167,6169,6173,6174,6176,6181,6182,6184,6186,6188,6190,6192,6193,6194,6197,6198,6199,6201,6202,6203,6209,6210,6213,6215,6216,6218,6219,6220,6227,6228,6230,6231,6234,6236,6239,6240,6244,6245,6251,6253,6254,6255,6256,6257,6262,6263,6265,6266,6271,6273,6275,6278,6281,6285,6286,6288,6290,6291,6293,6295,6298,6301,6302,6305,6311,6312,6315,6316,6322,6329,6332,6335,6338,6342,6343,6344,6346,6348,6350,6353,6354,6355,6359,6360,6362,6363,6367,6369,6370,6371,6384,6385,6387,6389,6392,6396,6399,6403,6404,6405,6407,6408,6409,6411,6416,6419,6420,6422,6429,6431,6435,6436,6445,6448,6451,6453,6455,6458,6459,6460,6468,6469,6470,6478,6479,6480,6481,6482,6484,6486,6487,6491,6496,6497,6498,6503,6504,6507,6509,6512,6517,6518,6521,6522,6523,6529,6530,6532,6535,6537,6540,6541,6542,6543,6544,6545,6549,6551,6553,6559,6562,6564,6565,6569,6571,6576,6579,6583,6586,6587,6590,6591,6593,6598,6600,6602,6604,6605,6613,6616,6621,6627,6628,6629,6630,6631,6632,6633,6634,6636,6639,6641,6643,6645,6647,6649,6650,6651,6652,6654,6656,6657,6661,6662,6663,6666,6667,6672,6678,6679,6682,6683,6686,6689,6690,6693,6694,6695,6696,6697,6701,6704,6705,6706,6707,6709,6711,6713,6714,6715,6722,6725,6731,6734,6735,6736,6739,6741,6742,6746,6747,6749,6751,6752,6754,6755,6756,6757,6758,6761,6762,6767,6769,6771,6774,6775,6776,6777,6781,6782,6783,6784,6786,6791,6797,6798,6800,6803,6808,6809,6817,6818,6819,6823,6831,6832,6833,6839,6840,6841,6842,6844,6848,6850,6852,6859,6861,6863,6864,6865,6867,6869,6870,6874,6875,6878,6881,6882,6885,6886,6887,6890,6891,6892,6898,6906,6908,6912,6916,6920,6921,6922,6923,6930,6932,6934,6937,6939,6940,6945,6955,6957,6959,6960,6961,6962,6963,6964,6967,6969,6973,6977,6979,6980,6982,6985,6989,6990,6992,6993,6994,6995,6997,6999,7004,7006,7007,7010,7011,7016,7020,7021,7024,7027,7028,7030,7032,7033,7034,7038,7043,7045,7046,7048,7050,7051,7052,7053,7059,7066,7067,7073,7074,7077,7079,7081,7082,7083,7085,7087,7089,7091,7092,7094,7099,7104,7105,7107,7109,7111,7113,7119,7121,7123,7124,7125,7126,7128,7130,7131,7134,7135,7136,7139,7140,7142,7143,7145,7146,7148,7149,7150,7151,7152,7154,7155,7160,7161,7163,7164,7166,7167,7168,7169,7170,7171,7172,7173,7175,7176,7179,7180,7182,7188,7194,7195,7197,7201,7203,7210,7211,7212,7215,7217,7221,7222,7232,7234,7239,7240,7242,7245,7246,7250,7251,7255,7256,7258,7259,7267,7269,7272,7276,7279,7283,7293,7294,7296,7298,7300,7301,7304,7307,7310,7312,7313,7314,7316,7320,7326,7332,7333,7343,7346,7350,7357,7358,7361,7363,7365,7367,7368,7369,7370,7372,7375,7376,7380,7381,7382,7386,7388,7392,7393,7394,7397,7399,7403,7404,7406,7409,7414,7416,7420,7425,7428,7430,7431,7433,7434,7439,7441,7445,7448,7451,7452,7453,7457,7458,7460,7462,7464,7465,7468,7470,7472,7478,7479,7480,7484,7486,7487,7488,7491,7493,7494,7497,7499,7508,7511,7513,7519,7520,7523,7524,7525,7532,7540,7542,7543,7544,7547,7550,7551,7554,7555,7560,7567,7574,7576,7577,7578,7581,7583,7585,7586,7587,7591,7593,7596,7597,7598,7601,7603,7605,7607,7609,7611,7613,7617,7618,7621,7629,7630,7631,7632,7635,7636,7638,7640,7648,7649,7651,7653,7655,7656,7657,7660,7661,7662,7669,7670,7672,7676,7678,7689,7690,7691,7695,7700,7706,7712,7713,7719,7723,7727,7728,7729,7733,7734,7736,7737,7739,7741,7747,7748,7753,7756,7758,7759,7760,7761,7762,7766,7767,7768,7770,7771,7772,7773,7774,7776,7777,7778,7779,7780,7781,7782,7783,7784,7786,7788,7790,7792,7796,7798,7800,7803,7804,7807,7808,7809,7813,7814,7815,7818,7821,7828,7832,7837,7841,7846,7850,7851,7854,7855,7856,7863,7866,7869,7874,7880,7882,7883,7884,7886,7887,7895,7896,7904,7910,7913,7917,7927,7930,7931,7933,7937,7938,7940,7941,7942,7944,7945,7946,7950,7951,7957,7963,7965,7969,7970,7972,7973,7979,7984,7988,7992,7995,7999,8002,8006,8009,8014,8015,8021,8022,8023,8025,8029,8031,8032,8034,8039,8041,8044,8045,8048,8049,8052,8055,8056,8057,8064,8065,8066,8067,8069,8070,8073,8078,8080,8082,8083,8085,8087,8089,8092,8098,8100,8102,8103,8110,8112,8113,8114,8118,8120,8124,8126,8130,8131,8133,8136,8137,8138,8142,8143,8147,8153,8157,8162,8167,8169,8171,8172,8173,8175,8178,8179,8181,8186,8195,8201,8202,8205,8206,8207,8211,8216,8222,8224,8225,8226,8227,8228,8229,8230,8231,8233,8234,8239,8240,8241,8244,8245,8251,8252,8255,8256,8258,8259,8261,8263,8265,8267,8269,8270,8271,8274,8275,8276,8278,8279,8280,8282,8283,8287,8289,8292,8295,8296,8298,8299,8302,8304,8307,8308,8311,8314,8315,8316,8319,8322,8327,8328,8329,8330,8333,8339,8340,8341,8342,8343,8353,8354,8355,8356,8357,8360,8361,8362,8363,8365,8366,8367,8368,8372,8373,8374,8379,8380,8381,8383,8387,8389,8392,8393,8396,8397,8401,8405,8406,8412,8414,8416,8419,8421,8422,8427,8430,8436,8440,8442,8444,8446,8448,8449,8450,8452,8453,8456,8459,8460,8463,8466,8469,8472,8473,8474,8478,8482,8485,8490,8499,8504,8505,8506,8507,8510,8515,8516,8518,8519,8520,8525,8527,8528,8529,8530,8533,8539,8540,8541,8542,8543,8545,8551,8554,8557,8558,8560,8563,8565,8567,8568,8573,8574,8577,8578,8581,8583,8585,8588,8591,8595,8608,8609,8610,8611,8612,8616,8617,8620,8621,8622,8623,8624,8628,8635,8636,8637,8638,8640,8642,8643,8646,8647,8649,8650,8652,8655,8656,8659,8660,8662,8664,8666,8673,8681,8683,8684,8686,8692,8696,8697,8701,8702,8703,8704,8707,8708,8709,8710,8713,8715,8716,8721,8727,8728,8734,8735,8741,8747,8750,8754,8755,8756,8759,8765,8768,8770,8776,8778,8784,8785,8793,8794,8795,8796,8799,8803,8806,8808,8810,8811,8812,8814,8815,8816,8822,8825,8828,8829,8830,8831,8832,8833,8834,8835,8836,8841,8844,8846,8849,8850,8851,8852,8854,8855,8861,8865,8868,8869,8872,8873,8875,8881,8882,8883,8885,8886,8888,8894,8895,8899,8900,8901,8904,8906,8907,8908,8914,8915,8916,8917,8918,8921,8923,8926,8927,8928,8929,8932,8933,8934,8935,8936,8938,8941,8946,8951,8956,8957,8964,8965,8966,8967,8971,8972,8976,8983,8987,8989,8990,8991,8994,8995,9003,9004,9007,9014,9015,9019,9021,9022,9026,9027,9028,9029,9034,9041,9048,9050,9051,9054,9055,9057,9058,9060,9062,9064,9066,9068,9070,9072,9074,9075,9085,9088,9096,9097,9098,9099,9102,9103,9107,9110,9112,9114,9115,9117,9118,9129,9135,9139,9142,9144,9147,9148,9149,9152,9154,9156,9159,9160,9161,9162,9167,9170,9177,9180,9181,9183,9184,9186,9188,9189,9193,9195,9198,9199,9201,9202,9204,9210,9213,9216,9219,9220,9224,9228,9229,9232,9236,9241,9250,9253,9254,9255,9270,9278,9284,9285,9292,9297,9300,9309,9314,9319,9320,9321,9322,9324,9330,9331,9335,9337,9338,9340,9342,9343,9344,9347,9348,9349,9350,9351,9355,9357,9361,9362,9363,9364,9365,9368,9369,9371,9372,9374,9378,9386,9391,9392,9394,9398,9400,9404,9405,9409,9411,9424,9425,9427,9434,9436,9437,9438,9439,9440,9442,9446,9449,9450,9451,9455,9456,9457,9458,9461,9462,9464,9468,9469,9470,9474,9475,9476,9477,9479,9480,9481,9483,9485,9488,9489,9490,9491,9492,9500,9503,9505,9507,9513,9516,9517,9522,9528,9529,9530,9531,9532,9534,9536,9537,9540,9542,9544,9548,9550,9554,9557,9562,9563,9566,9567,9568,9570,9575,9579,9582,9583,9585,9603,9604,9605,9606,9607,9610,9615,9618,9620,9621,9623,9625,9637,9638,9649,9652,9656,9658,9659,9663,9666,9672,9673,9674,9675,9677,9679,9681,9682,9684,9685,9687,9688,9689,9691,9695,9697,9699,9701,9704,9706,9708,9709,9712,9713,9714,9715,9716,9718,9719,9721,9722,9724,9725,9727,9728,9746,9747,9748,9749,9751,9752,9757,9758,9759,9762,9766,9769,9775,9781,9789,9791,9794,9796,9800,9804,9806,9807,9808,9809,9810,9813,9814,9816,9818,9819,9820,9821,9823,9824,9826,9829,9833,9834,9835,9836,9837,9838,9839,9840,9841,9843,9846,9849,9851,9856,9857,9861,9862,9870,9873,9874,9875,9877,9878,9881,9885,9888,9889,9890,9891,9896,9898,9900,9901,9902,9904,9905,9908,9909,9910,9912,9913,9914,9915,9916,9928,9930,9931,9933,9934,9935,9937,9938,9940,9945,9947,9948,9949,9950,9951,9952,9954,9957,9958,9960,9968,9970,9973,9974,9975,9976,9977,9979,9980,9981,9988,9991,9993,9996,10001,10002,10005,10007,10010,10011,10016,10017,10018,10019,10020,10022,10028,10030,10031,10036,10040,10041,10043,10044,10045,10050,10057,10058,10060,10061,10064,10068,10071,10072,10074,10076,10078,10085,10087,10095,10097,10099,10100,10106,10108,10115,10117,10118,10119,10123,10124,10125,10128,10129,10130,10131,10135,10137,10140,10143,10144,10145,10148,10150,10151,10154,10156,10161,10163,10168,10170,10172,10177,10184,10193,10194,10202,10204,10208,10212,10214,10221,10223,10224,10225,10227,10230,10231,10232,10233,10239,10245,10246,10248,10251,10262,10264,10267,10270,10273,10280,10281,10282,10284,10286,10287,10288,10290,10291,10295,10296,10303,10304,10306,10307,10309,10310,10313,10319,10320,10322,10323,10325,10326,10327,10328,10330,10332,10333,10336,10338,10339,10340,10342,10348,10349,10351,10352,10353,10360,10361,10363,10364,10365,10369,10370,10371,10376,10380,10381,10384,10385,10387,10388,10395,10397,10398,10400,10404,10406,10407,10410,10415,10416,10417,10419,10420,10422,10424,10426,10427,10428,10429,10430,10435,10440,10442,10447,10452,10453,10456,10465,10467,10470,10472,10474,10475,10477,10482,10484,10485,10490,10493,10494,10495,10500,10506,10507,10510,10511,10514,10516,10517,10518,10521,10526,10527,10529,10531,10532,10533,10535,10537,10538,10540,10541,10544,10545,10546,10547,10550,10551,10554,10555,10557,10564,10565,10566,10567,10568,10576,10579,10580,10581,10582,10583,10585,10589,10594,10596,10605,10612,10613,10614,10618,10622,10623,10625,10626,10627,10628,10631,10636,10637,10638,10639,10641,10643,10646,10649,10650,10657,10658,10661,10664,10665,10669,10671,10672,10673,10674,10675,10676,10677,10678,10679,10684,10685,10686,10692,10694,10701,10703,10705,10706,10707,10710,10713,10718,10719,10721,10726,10729,10730,10732,10734,10738,10749,10750,10751,10754,10757,10758,10761,10765,10767,10769,10772,10774,10777,10781,10782,10783,10784,10789,10791,10794,10795,10796,10797,10798,10799,10802,10807,10809,10810,10811,10816,10817,10820,10822,10825,10827,10831,10833,10836,10837,10841,10843,10845,10847,10852,10854,10856,10858,10860,10863,10864,10865,10868,10871,10872,10876,10877,10881,10885,10886,10887,10889,10890,10893,10898,10900,10901,10902,10903,10906,10911,10912,10913,10916,10919,10921,10922,10924,10926,10927,10928,10929,10939,10940,10941,10943,10944,10946,10956,10959,10960,10962,10963,10965,10969,10971,10972,10974,10975,10979,10980,10981,10984,10986,10989,10991,10992,10994,10997,11011,11014,11018,11019,11020,11021,11025,11027,11034,11035,11038,11040,11042,11044,11045,11046,11049,11056,11057,11058,11059,11062,11063,11066,11068,11069,11071,11073,11076,11077,11078,11079,11083,11086,11088,11089,11090,11093,11094,11095,11099,11100,11103,11107,11108,11109,11110,11112,11113,11114,11119,11120,11121,11124,11129,11131,11132,11133,11134,11135,11137,11138,11141,11143,11145,11152,11153,11154,11156,11165,11170,11171,11174,11175,11176,11177,11179,11181,11182,11184,11187,11189,11191,11192,11195,11198,11199,11200,11203,11206,11208,11209,11211,11216,11217,11218,11222,11223,11224,11228,11230,11233,11236,11239,11240,11241,11242,11244,11246,11250,11251,11253,11256,11261,11268,11269,11273,11280,11287,11292,11295,11300,11301,11315,11319,11326,11327,11328,11333,11334,11335,11336,11338,11341,11343,11349,11352,11356,11358,11360,11361,11367,11378,11379,11381,11382,11387,11389,11390,11391,11392,11393,11396,11401,11402,11404,11406,11408,11412,11417,11418,11419,11421,11422,11430,11433,11434,11435,11436,11439,11440,11441,11442,11445,11447,11448,11449,11450,11451,11454,11457,11462,11465,11468,11470,11473,11476,11477,11484,11488,11492,11495,11496,11498,11500,11503,11511,11513,11515,11518,11519,11521,11522,11524,11525,11526,11528,11530,11532,11534,11535,11536,11537,11538,11539,11540,11541,11545,11546,11549,11550,11551,11552,11553,11559,11565,11569,11572,11575,11576,11579,11580,11581,11583,11585,11586,11589,11592,11597,11599,11600,11605,11610,11613,11614,11615,11616,11617,11619,11623,11626,11629,11633,11634,11635,11642,11645,11648,11654,11656,11658,11662,11664,11666,11674,11681,11683,11691,11694,11695,11697,11703,11704,11706,11711,11716,11718,11720,11722,11729,11730,11733,11736,11737,11739,11740,11743,11744,11748,11750,11754,11758,11760,11761,11762,11763,11765,11766,11767,11769,11772,11775,11782,11786,11791,11795,11804,11805,11807,11808,11810,11811,11814,11818,11819,11821,11824,11831,11833,11834,11836,11837,11842,11848,11850,11854,11856,11857,11860,11865,11873,11874,11880,11881,11887,11894,11895,11897,11898,11899,11901,11908,11909,11912,11913,11915,11916,11917,11918,11921,11928,11929,11930,11932,11938,11941,11942,11945,11946,11947,11952,11954,11955,11956,11964,11966,11976,11980,11981,11982,11983,11993,11996,11997,11998,11999,12002,12006,12009,12012,12016,12018,12024,12025,12026,12027,12032,12033,12036,12039,12042,12044,12045,12050,12053,12054,12056,12059,12060,12071,12072,12073,12074,12080,12084,12086,12091,12093,12098,12103,12106,12123,12127,12130,12132,12141,12148,12157,12159,12162,12163,12166,12167,12169,12173,12175,12176,12178,12181,12189,12190,12191,12194,12198,12208,12209,12210,12212,12219,12221,12222,12226,12228,12231,12232,12241,12244,12246,12251,12253,12256,12257,12258,12260,12262,12263,12264,12265,12266,12267,12274,12276,12281,12282,12284,12288,12289,12290,12292,12293,12294,12296,12298,12301,12308,12313,12317,12318,12319,12326,12332,12333,12335,12338,12340,12344,12346,12347,12351,12358,12360,12362,12367,12369,12371,12377,12385,12389,12390,12392,12395,12398,12401,12403,12406,12408,12409,12412,12418,12419,12421,12422,12423,12425,12426,12427,12428,12429,12430,12435,12436,12437,12440,12441,12443,12445,12446,12452,12456,12458,12460,12469,12471,12472,12473,12475,12476,12480,12492,12493,12496,12498,12499,12501,12502,12504,12505,12506,12509,12515,12518,12519,12527,12531,12534,12538,12543,12545,12546,12555,12558,12560,12564,12567,12570,12571,12573,12574,12577,12580,12581,12584,12586,12587,12588,12591,12595,12598,12600,12603,12604,12607,12609,12610,12613,12614,12624,12625,12629,12631,12632,12636,12637,12638,12640,12642,12646,12650,12651,12663,12667,12668,12669,12672,12676,12677,12682,12686,12687,12688,12690,12692,12693,12696,12702,12710,12712,12714,12718,12721,12723,12726,12728,12729,12730,12731,12732,12734,12745,12756,12758,12760,12763,12766,12770,12777,12786,12787,12799,12800,12809,12811,12823,12833,12834,12835,12837,12840,12849,12851,12853,12854,12855,12856,12866,12869,12871,12880,12891,12892,12894,12901,12902,12903,12909,12911,12913,12916,12918,12919,12920,12921,12923,12924,12927,12931,12934,12939,12941,12943,12947,12948,12950,12952,12958,12962,12963,12967,12968,12970,12976,12991,12995,12996,12998,12999,13000,13002,13008,13009,13012,13015,13018,13019,13021,13022,13025,13027,13028,13030,13032,13039,13044,13051,13055,13056,13057,13060,13067,13068,13069,13073,13085,13089,13090,13094,13100,13107,13108,13111,13113,13115,13117,13119,13122,13126,13130,13134,13140,13145,13146,13152,13157,13161,13162,13168,13170,13173,13174,13177,13178,13180,13181,13189,13190,13195,13197,13206,13207,13212,13218,13219,13220,13221,13237,13238,13239,13250,13252,13253,13256,13260,13267,13273,13275,13277,13279,13280,13282,13285,13286,13289,13294,13296,13301,13305,13308,13309,13314,13318,13326,13332,13333,13337,13344,13347,13353,13356,13357,13360,13361,13365,13366,13369,13376,13377,13381,13383,13385,13389,13390,13393,13394,13396,13399,13400,13409,13410,13411,13417,13422,13423,13425,13431,13433,13435,13437,13439,13441,13446,13449,13466,13467,13474,13476,13479,13481,13482,13491,13492,13496,13497,13502,13508,13509,13511,13512,13513,13515,13517,13519,13522,13524,13527,13528,13529,13532,13533,13536,13537,13538,13541,13543,13544,13549,13553,13554,13556,13557,13570,13571,13573,13574,13587,13591,13598,13599,13603,13618,13628,13629,13630,13637,13640,13645,13651,13654,13656,13674,13675,13678,13679,13681,13685,13686,13689,13690,13692,13698,13704,13708,13709,13710,13714,13723,13729,13731,13732,13734,13735,13737,13738,13740,13742,13744,13745,13746,13749,13750,13754,13760,13762,13764,13768,13772,13779,13781,13783,13785,13787,13789,13792,13793,13797,13798,13803,13810,13819,13820,13821,13824,13828,13829,13833,13836,13840,13841,13842,13843,13846,13847,13848,13854,13855,13859,13863,13865,13869,13872,13877,13881,13884,13888,13892,13896,13901,13902,13907,13908,13911,13914,13917,13918,13922,13923,13924,13926,13927,13928,13929,13931,13939,13940,13944,13946,13948,13952,13954,13956,13960,13962,13967,13968,13976,13979,13992,13993,13994,13995,14001,14005,14007,14008,14018,14019,14025,14027,14028,14030,14034,14038,14046,14048,14050,14051,14052,14055,14056,14059,14067,14072,14074,14076,14077,14083,14092,14102,14103,14105,14106,14107,14109,14110,14115,14119,14123,14125,14126,14128,14129,14130,14136,14142,14149,14151,14155,14156,14158,14160,14169,14170,14175,14177,14180,14183,14185,14190,14193,14199,14203,14207,14213,14214,14221,14223,14224,14226,14227,14232,14233,14234,14237,14238,14239,14241,14244,14248,14249,14258,14264,14272,14274,14275,14280,14284,14289,14296,14300,14304,14305,14310,14312,14317,14322,14327,14329,14334,14340,14341,14345,14351,14358,14359,14360,14368,14371,14374,14376,14384,14387,14388,14395,14397,14399,14403,14404,14412,14413,14419,14420,14423,14424,14428,14429,14431,14432,14433,14436,14438,14439,14440,14445,14455,14462,14464,14467,14470,14471,14473,14480,14482,14483,14488,14492,14495,14502,14506,14508,14511,14512,14519,14521,14525,14526,14527,14528,14530,14534,14536,14538,14539,14540,14541,14543,14545,14550,14554,14558,14562,14564,14567,14571,14572,14573,14576,14583,14587,14593,14600,14601,14602,14608,14611,14614,14615,14618,14619,14620,14624,14628,14635,14642,14647,14651,14654,14659,14673,14674,14675,14681,14682,14690,14692,14701,14703,14707,14708,14710,14725,14726,14727,14728,14739,14741,14758,14768,14769,14773,14778,14779,14780,14783,14786,14787,14789,14791,14792,14793,14795,14796,14798,14799,14806,14807,14809,14813,14814,14818,14820,14821,14825,14832,14835,14840,14842,14843,14846,14848,14849,14851,14853,14854,14855,14856,14857,14859,14860,14861,14862,14864,14866,14870,14878,14879,14883,14885,14886,14888,14890,14891,14894,14895,14904,14905,14911,14914,14921,14924,14928,14931,14933,14936,14937,14938,14941,14944,14948,14954,14971,14973,14975,14976,14978,14986,14987,14995,15000,15001,15003,15005,15006,15008,15010,15019,15021,15022,15024,15025,15026,15027,15030,15035,15041,15042,15043,15044,15046,15055,15061,15069,15070,15072,15078,15080,15082,15088,15090,15096,15097,15101,15107,15108,15111,15113,15116,15117,15121,15123,15125,15143,15145,15149,15152,15154,15156,15158,15159,15160,15162,15169,15176,15177,15178,15189,15193,15194,15196,15197,15199,15208,15209,15217,15220,15229,15234,15236,15240,15244,15246,15247,15251,15252,15260,15261,15262,15267,15268,15269,15270,15272,15277,15279,15290,15291,15292,15293,15294,15301,15304,15305,15307,15311,15315,15318,15322,15323,15324,15328,15331,15338,15342,15354,15361,15362,15364,15367,15374,15381,15383,15387,15394,15397,15400,15402,15404,15406,15409,15411,15424,15429,15432,15440,15442,15448,15449,15450,15453,15460,15462,15465,15473,15484,15485,15487,15495,15499,15500,15501,15503,15504,15505,15508,15509,15511,15513,15520,15527,15530,15542,15546,15547,15552,15554,15563,15568,15570,15571,15576,15579,15581,15582,15584,15586,15590,15591,15595,15597,15598,15600,15601,15604,15607,15609,15614,15624,15631,15632,15634,15641,15643,15648,15649,15650,15651,15653,15654,15659,15661,15668,15670,15671,15674,15675,15677,15681,15690,15692,15693,15695,15702,15708,15718,15722,15723,15724,15725,15727,15729,15734,15736,15741,15743,15745,15751,15753,15754,15755,15759,15762,15767,15768,15772,15775,15777,15778,15779,15780,15784,15786,15788,15789,15791,15792,15794,15795,15798,15800,15803,15806,15807,15817,15818,15819,15823,15824,15829,15832,15840,15842,15847,15852,15854,15856,15858,15861,15865,15867,15872,15873,15874,15875,15876,15880,15881,15889,15891,15892,15893,15895,15899,15904,15910,15911,15913,15914,15915,15917,15920,15922,15923,15924,15927,15928,15929,15930,15932,15933,15935,15937,15938,15939,15945,15946,15949,15951,15952,15953,15955,15956,15957,15961,15963,15969,15979,15981,15982,15984,15985,15989,15990,15996,16001,16002,16004,16011,16012,16013,16014,16015,16016,16017,16019,16020,16023,16026,16027,16028,16031,16032,16033,16034,16035,16036,16037,16038,16039,16043,16044,16051,16052,16053,16057,16059,16060,16062) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T19:37:39.710422Z",
     "start_time": "2025-06-30T19:37:39.698422Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mapping = {'Leukemia': 0, 'Lymphoma': 1, 'CNS': 2, 'Breast': 3, 'Bladder': 4, 'Pancreas': 5, 'Colorectal': 6, 'Lung': 7, 'Ovary': 8, 'Renal': 9}\n",
    "mapping[y_gcm[0]]\n",
    "\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T19:37:39.882420Z",
     "start_time": "2025-06-30T19:37:39.856422Z"
    }
   },
   "cell_type": "code",
   "source": "y_gcm",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            Breast\n",
       "1            Breast\n",
       "2            Breast\n",
       "3            Breast\n",
       "4            Breast\n",
       "           ...     \n",
       "185    Mesothelioma\n",
       "186             CNS\n",
       "187             CNS\n",
       "188             CNS\n",
       "189             CNS\n",
       "Name: class, Length: 190, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## <font color = \"darkgreen\">3- Traitement des valeurs nulles/vides</font>"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Analysez tous les jeux de données, vérifiez si dans certaines observations il existe des valeurs nulles/vides\n",
    "* Renvoyez le nombre d'obervations qui y sont concernées"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T19:37:40.021420Z",
     "start_time": "2025-06-30T19:37:40.010420Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_nan_count(X):\n",
    "    filtre = X.apply(lambda col: col.astype(str).str.contains(r'[nN][aA][nN]|[nN][aA][nN]\\d+|\\d+[nN][aA][nN]', regex=True))\n",
    "    nb_lignes_avec_nan = filtre.any(axis=1).sum()\n",
    "    return nb_lignes_avec_nan\n"
   ],
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T19:37:55.241234Z",
     "start_time": "2025-06-30T19:37:40.228443Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"Nombre de lignes diabetes null/vide : {get_nan_count(X_diabetes)}\")\n",
    "print(f\"Nombre de lignes wrn null/vide : {get_nan_count(X_wrn)}\")\n",
    "print(f\"Nombre de lignes japanese null/vide : {get_nan_count(X_japanese)}\")\n",
    "print(f\"Nombre de lignes gcm null/vide : {get_nan_count(X_gcm)}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de lignes diabetes null/vide : 95\n",
      "Nombre de lignes wrn null/vide : 110\n",
      "Nombre de lignes japanese null/vide : 0\n",
      "Nombre de lignes gcm null/vide : 190\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Ces observations nécessitent l'une des opérations suivantes:\n",
    "* 1- remplacer les valeurs nulles/vides par la moyenne des valeures des observations pour la caratéristique concernée, à répéter pour chaque  caratéristique.\n",
    "* 2- même opération mais cette-fois ci en calculant le maximum\n",
    "* 3- même opération mais cette-fois ci en calculant le minimum\n",
    "* 4- supprimer les observations contenant les valeurs nulles/vides.\n",
    "\n",
    " Développez une fonction (qu'on appelera processNan) qui prend en paramètres le jeu de données (dataframe) et un entier, qu'on appelera typeOp, compris dans [1,4]\n",
    " En fonction de la valeur de typeOp, vous appliquez l'une des quatre opérations sur le jeu de données fourni en paramètre."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T19:37:55.459228Z",
     "start_time": "2025-06-30T19:37:55.448230Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T19:37:55.879230Z",
     "start_time": "2025-06-30T19:37:55.866228Z"
    }
   },
   "source": [
    "\n",
    "def remplacer_mean(X):\n",
    "    X = X.replace(to_replace=r'[nN][aA][nN]|[nN][aA][nN]\\d+|\\d+[nN][aA][nN]',\n",
    "                    value=np.nan,\n",
    "                    regex=True)\n",
    "\n",
    "    X = X.apply(pd.to_numeric, errors='coerce')\n",
    "    df_remplie = X.fillna(X.mean())\n",
    "    return df_remplie"
   ],
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T19:37:56.018229Z",
     "start_time": "2025-06-30T19:37:56.006238Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def remplacer_max(X):\n",
    "    X = X.replace(to_replace=r'[nN][aA][nN]|[nN][aA][nN]\\d+|\\d+[nN][aA][nN]',\n",
    "                    value=np.nan,\n",
    "                    regex=True)\n",
    "\n",
    "    X = X.apply(pd.to_numeric, errors='coerce')\n",
    "    df_remplie = X.fillna(X.max())\n",
    "    return df_remplie"
   ],
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T19:37:56.173231Z",
     "start_time": "2025-06-30T19:37:56.159229Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def remplacer_min(X):\n",
    "    X = X.replace(to_replace=r'[nN][aA][nN]|[nN][aA][nN]\\d+|\\d+[nN][aA][nN]',\n",
    "                    value=np.nan,\n",
    "                    regex=True)\n",
    "\n",
    "    X = X.apply(pd.to_numeric, errors='coerce')\n",
    "    df_remplie = X.fillna(X.max())\n",
    "    return df_remplie"
   ],
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T19:37:56.314232Z",
     "start_time": "2025-06-30T19:37:56.301231Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def remove_nan(X):\n",
    "    X = X.replace(to_replace=r'[nN][aA][nN]|[nN][aA][nN]\\d+|\\d+[nN][aA][nN]',\n",
    "                  value=np.nan,\n",
    "                  regex=True)\n",
    "    df_cleaned = X.dropna()\n",
    "    return df_cleaned"
   ],
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T19:37:56.453247Z",
     "start_time": "2025-06-30T19:37:56.444235Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def processNan(X, typeOp):\n",
    "    if typeOp == 1:\n",
    "        return remplacer_mean(X)\n",
    "    elif typeOp == 2:\n",
    "        return remplacer_max(X)\n",
    "    elif typeOp == 3:\n",
    "        return remplacer_min(X)\n",
    "    elif typeOp == 4:\n",
    "        return remove_nan(X)\n",
    "    else:\n",
    "        raise ValueError(\"typeOp doit être compris entre 1 et 4\")"
   ],
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T19:37:56.656330Z",
     "start_time": "2025-06-30T19:37:56.582781Z"
    }
   },
   "cell_type": "code",
   "source": "processNan(X_diabetes, 1)",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin   BMI  \\\n",
       "0              6  148.000000             72           35.0    0.000000  33.6   \n",
       "1              1   85.000000             66           29.0    0.000000  26.6   \n",
       "2              8  118.508174             64            0.0    0.000000  23.3   \n",
       "3              1   89.000000             66           23.0   94.000000  28.1   \n",
       "4              0  137.000000             40           35.0  168.000000  43.1   \n",
       "..           ...         ...            ...            ...         ...   ...   \n",
       "763           10  101.000000             76           48.0   77.601333  32.9   \n",
       "764            2  122.000000             70           27.0    0.000000  36.8   \n",
       "765            5  121.000000             72           23.0  112.000000  26.2   \n",
       "766            1  126.000000             60            0.0    0.000000  30.1   \n",
       "767            1   93.000000             70           31.0    0.000000  30.4   \n",
       "\n",
       "     DiabetesPedigreeFunction  Age  \n",
       "0                       0.627   50  \n",
       "1                       0.351   31  \n",
       "2                       0.672   32  \n",
       "3                       0.167   21  \n",
       "4                       2.288   33  \n",
       "..                        ...  ...  \n",
       "763                     0.171   63  \n",
       "764                     0.340   27  \n",
       "765                     0.245   30  \n",
       "766                     0.349   47  \n",
       "767                     0.315   23  \n",
       "\n",
       "[768 rows x 8 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148.000000</td>\n",
       "      <td>72</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>66</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>118.508174</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>66</td>\n",
       "      <td>23.0</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>40</td>\n",
       "      <td>35.0</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>10</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>76</td>\n",
       "      <td>48.0</td>\n",
       "      <td>77.601333</td>\n",
       "      <td>32.9</td>\n",
       "      <td>0.171</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>2</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>70</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>36.8</td>\n",
       "      <td>0.340</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>5</td>\n",
       "      <td>121.000000</td>\n",
       "      <td>72</td>\n",
       "      <td>23.0</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>26.2</td>\n",
       "      <td>0.245</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>1</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>60</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.349</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>1</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>70</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.4</td>\n",
       "      <td>0.315</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows × 8 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T19:37:57.310332Z",
     "start_time": "2025-06-30T19:37:56.924335Z"
    }
   },
   "cell_type": "code",
   "source": "processNan(X_wrn, 1)\n",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         V1     V2     V3     V4     V5     V6     V7     V8     V9    V10  \\\n",
       "0     0.438  0.498  3.625  3.645  5.000  2.918  5.000  2.351  2.332  2.643   \n",
       "1     0.438  0.498  3.625  3.648  5.000  2.918  5.000  2.637  2.332  2.649   \n",
       "2     0.438  0.498  3.625  3.629  5.000  2.918  5.000  2.637  2.334  2.643   \n",
       "3     0.437  0.501  3.625  3.626  5.000  2.918  5.000  2.353  2.334  2.642   \n",
       "4     0.438  0.498  3.626  3.629  5.000  2.918  5.000  2.640  2.334  2.639   \n",
       "...     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "5451  0.910  5.000  3.997  2.785  2.770  2.572  2.433  1.087  1.772  1.040   \n",
       "5452  0.926  5.000  4.015  2.792  2.777  2.571  1.768  1.071  1.762  1.021   \n",
       "5453  0.937  5.000  4.034  2.799  2.784  2.571  1.754  1.053  1.752  1.002   \n",
       "5454  0.945  4.052  4.052  2.809  2.791  2.441  1.757  1.034  1.743  0.983   \n",
       "5455  0.950  4.066  5.000  2.819  2.798  2.570  2.422  1.016  1.739  0.964   \n",
       "\n",
       "      ...    V15    V16    V17    V18    V19    V20    V21    V22    V23  \\\n",
       "0     ...  1.744  0.593  0.502  0.493  0.504  0.445  0.431  0.444  0.440   \n",
       "1     ...  1.744  0.592  0.502  0.493  0.504  0.449  0.431  0.444  0.443   \n",
       "2     ...  1.744  0.593  0.502  0.493  0.504  0.449  0.431  0.444  0.446   \n",
       "3     ...  1.744  0.593  0.502  0.493  0.504  0.449  0.431  0.444  0.444   \n",
       "4     ...  1.744  0.592  0.502  0.493  0.504  0.449  0.431  0.444  0.441   \n",
       "...   ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "5451  ...  5.000  0.660  0.648  0.657  0.686  5.000  1.045  5.000  5.000   \n",
       "5452  ...  0.894  0.652  0.640  0.649  1.593  1.616  1.058  5.000  5.000   \n",
       "5453  ...  0.873  0.648  0.633  0.642  0.741  5.000  1.065  5.000  5.000   \n",
       "5454  ...  5.000  0.641  0.626  0.635  0.754  5.000  1.076  5.000  5.000   \n",
       "5455  ...  0.854  0.635  0.618  0.628  0.776  5.000  1.083  5.000  5.000   \n",
       "\n",
       "        V24  \n",
       "0     0.429  \n",
       "1     0.429  \n",
       "2     0.429  \n",
       "3     0.429  \n",
       "4     0.429  \n",
       "...     ...  \n",
       "5451  1.562  \n",
       "5452  1.085  \n",
       "5453  1.105  \n",
       "5454  1.118  \n",
       "5455  1.168  \n",
       "\n",
       "[5456 rows x 24 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V15</th>\n",
       "      <th>V16</th>\n",
       "      <th>V17</th>\n",
       "      <th>V18</th>\n",
       "      <th>V19</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.438</td>\n",
       "      <td>0.498</td>\n",
       "      <td>3.625</td>\n",
       "      <td>3.645</td>\n",
       "      <td>5.000</td>\n",
       "      <td>2.918</td>\n",
       "      <td>5.000</td>\n",
       "      <td>2.351</td>\n",
       "      <td>2.332</td>\n",
       "      <td>2.643</td>\n",
       "      <td>...</td>\n",
       "      <td>1.744</td>\n",
       "      <td>0.593</td>\n",
       "      <td>0.502</td>\n",
       "      <td>0.493</td>\n",
       "      <td>0.504</td>\n",
       "      <td>0.445</td>\n",
       "      <td>0.431</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.438</td>\n",
       "      <td>0.498</td>\n",
       "      <td>3.625</td>\n",
       "      <td>3.648</td>\n",
       "      <td>5.000</td>\n",
       "      <td>2.918</td>\n",
       "      <td>5.000</td>\n",
       "      <td>2.637</td>\n",
       "      <td>2.332</td>\n",
       "      <td>2.649</td>\n",
       "      <td>...</td>\n",
       "      <td>1.744</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.502</td>\n",
       "      <td>0.493</td>\n",
       "      <td>0.504</td>\n",
       "      <td>0.449</td>\n",
       "      <td>0.431</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.443</td>\n",
       "      <td>0.429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.438</td>\n",
       "      <td>0.498</td>\n",
       "      <td>3.625</td>\n",
       "      <td>3.629</td>\n",
       "      <td>5.000</td>\n",
       "      <td>2.918</td>\n",
       "      <td>5.000</td>\n",
       "      <td>2.637</td>\n",
       "      <td>2.334</td>\n",
       "      <td>2.643</td>\n",
       "      <td>...</td>\n",
       "      <td>1.744</td>\n",
       "      <td>0.593</td>\n",
       "      <td>0.502</td>\n",
       "      <td>0.493</td>\n",
       "      <td>0.504</td>\n",
       "      <td>0.449</td>\n",
       "      <td>0.431</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.446</td>\n",
       "      <td>0.429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.437</td>\n",
       "      <td>0.501</td>\n",
       "      <td>3.625</td>\n",
       "      <td>3.626</td>\n",
       "      <td>5.000</td>\n",
       "      <td>2.918</td>\n",
       "      <td>5.000</td>\n",
       "      <td>2.353</td>\n",
       "      <td>2.334</td>\n",
       "      <td>2.642</td>\n",
       "      <td>...</td>\n",
       "      <td>1.744</td>\n",
       "      <td>0.593</td>\n",
       "      <td>0.502</td>\n",
       "      <td>0.493</td>\n",
       "      <td>0.504</td>\n",
       "      <td>0.449</td>\n",
       "      <td>0.431</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.438</td>\n",
       "      <td>0.498</td>\n",
       "      <td>3.626</td>\n",
       "      <td>3.629</td>\n",
       "      <td>5.000</td>\n",
       "      <td>2.918</td>\n",
       "      <td>5.000</td>\n",
       "      <td>2.640</td>\n",
       "      <td>2.334</td>\n",
       "      <td>2.639</td>\n",
       "      <td>...</td>\n",
       "      <td>1.744</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.502</td>\n",
       "      <td>0.493</td>\n",
       "      <td>0.504</td>\n",
       "      <td>0.449</td>\n",
       "      <td>0.431</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.441</td>\n",
       "      <td>0.429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5451</th>\n",
       "      <td>0.910</td>\n",
       "      <td>5.000</td>\n",
       "      <td>3.997</td>\n",
       "      <td>2.785</td>\n",
       "      <td>2.770</td>\n",
       "      <td>2.572</td>\n",
       "      <td>2.433</td>\n",
       "      <td>1.087</td>\n",
       "      <td>1.772</td>\n",
       "      <td>1.040</td>\n",
       "      <td>...</td>\n",
       "      <td>5.000</td>\n",
       "      <td>0.660</td>\n",
       "      <td>0.648</td>\n",
       "      <td>0.657</td>\n",
       "      <td>0.686</td>\n",
       "      <td>5.000</td>\n",
       "      <td>1.045</td>\n",
       "      <td>5.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>1.562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5452</th>\n",
       "      <td>0.926</td>\n",
       "      <td>5.000</td>\n",
       "      <td>4.015</td>\n",
       "      <td>2.792</td>\n",
       "      <td>2.777</td>\n",
       "      <td>2.571</td>\n",
       "      <td>1.768</td>\n",
       "      <td>1.071</td>\n",
       "      <td>1.762</td>\n",
       "      <td>1.021</td>\n",
       "      <td>...</td>\n",
       "      <td>0.894</td>\n",
       "      <td>0.652</td>\n",
       "      <td>0.640</td>\n",
       "      <td>0.649</td>\n",
       "      <td>1.593</td>\n",
       "      <td>1.616</td>\n",
       "      <td>1.058</td>\n",
       "      <td>5.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>1.085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5453</th>\n",
       "      <td>0.937</td>\n",
       "      <td>5.000</td>\n",
       "      <td>4.034</td>\n",
       "      <td>2.799</td>\n",
       "      <td>2.784</td>\n",
       "      <td>2.571</td>\n",
       "      <td>1.754</td>\n",
       "      <td>1.053</td>\n",
       "      <td>1.752</td>\n",
       "      <td>1.002</td>\n",
       "      <td>...</td>\n",
       "      <td>0.873</td>\n",
       "      <td>0.648</td>\n",
       "      <td>0.633</td>\n",
       "      <td>0.642</td>\n",
       "      <td>0.741</td>\n",
       "      <td>5.000</td>\n",
       "      <td>1.065</td>\n",
       "      <td>5.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>1.105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5454</th>\n",
       "      <td>0.945</td>\n",
       "      <td>4.052</td>\n",
       "      <td>4.052</td>\n",
       "      <td>2.809</td>\n",
       "      <td>2.791</td>\n",
       "      <td>2.441</td>\n",
       "      <td>1.757</td>\n",
       "      <td>1.034</td>\n",
       "      <td>1.743</td>\n",
       "      <td>0.983</td>\n",
       "      <td>...</td>\n",
       "      <td>5.000</td>\n",
       "      <td>0.641</td>\n",
       "      <td>0.626</td>\n",
       "      <td>0.635</td>\n",
       "      <td>0.754</td>\n",
       "      <td>5.000</td>\n",
       "      <td>1.076</td>\n",
       "      <td>5.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>1.118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5455</th>\n",
       "      <td>0.950</td>\n",
       "      <td>4.066</td>\n",
       "      <td>5.000</td>\n",
       "      <td>2.819</td>\n",
       "      <td>2.798</td>\n",
       "      <td>2.570</td>\n",
       "      <td>2.422</td>\n",
       "      <td>1.016</td>\n",
       "      <td>1.739</td>\n",
       "      <td>0.964</td>\n",
       "      <td>...</td>\n",
       "      <td>0.854</td>\n",
       "      <td>0.635</td>\n",
       "      <td>0.618</td>\n",
       "      <td>0.628</td>\n",
       "      <td>0.776</td>\n",
       "      <td>5.000</td>\n",
       "      <td>1.083</td>\n",
       "      <td>5.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>1.168</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5456 rows × 24 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T19:37:57.822330Z",
     "start_time": "2025-06-30T19:37:57.748334Z"
    }
   },
   "cell_type": "code",
   "source": "processNan(X_japanese, 1)",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      utterance  frame  coefficient1  coefficient2  coefficient3  \\\n",
       "0             1      1      1.860936     -0.207383      0.261557   \n",
       "1             1      2      1.891651     -0.193249      0.235363   \n",
       "2             1      3      1.939205     -0.239664      0.258561   \n",
       "3             1      4      1.717517     -0.218572      0.217119   \n",
       "4             1      5      1.741191     -0.279891      0.196583   \n",
       "...         ...    ...           ...           ...           ...   \n",
       "9956         29      7      1.216506     -0.424432     -0.034349   \n",
       "9957         29      8      1.214579     -0.399925     -0.127891   \n",
       "9958         29      9      1.170031     -0.306025     -0.145534   \n",
       "9959         29     10      1.118108     -0.258605     -0.103982   \n",
       "9960         29     11      1.177449     -0.404080      0.052026   \n",
       "\n",
       "      coefficient4  coefficient5  coefficient6  coefficient7  coefficient8  \\\n",
       "0        -0.214562     -0.171253     -0.118167     -0.277557      0.025668   \n",
       "1        -0.249118     -0.112890     -0.112238     -0.311997     -0.027122   \n",
       "2        -0.291458     -0.041053     -0.102034     -0.383300      0.019013   \n",
       "3        -0.228186     -0.018608     -0.137624     -0.403318     -0.009643   \n",
       "4        -0.236377     -0.032012     -0.090612     -0.363134     -0.012571   \n",
       "...            ...           ...           ...           ...           ...   \n",
       "9956     -0.294238      0.166270     -0.397758      0.091827      0.023178   \n",
       "9957     -0.239583      0.221423     -0.374714      0.046216      0.051754   \n",
       "9958     -0.195854      0.276371     -0.346794      0.042325      0.031668   \n",
       "9959     -0.231256      0.344866     -0.223807     -0.014415     -0.046215   \n",
       "9960     -0.284812      0.429840     -0.203484     -0.072393     -0.080955   \n",
       "\n",
       "      coefficient9  coefficient10  coefficient11  coefficient12  \n",
       "0         0.126701      -0.306756      -0.213076       0.088728  \n",
       "1         0.171457      -0.289431      -0.247722       0.093011  \n",
       "2         0.169510      -0.314894      -0.227908       0.074638  \n",
       "3         0.164607      -0.323267      -0.210105       0.098098  \n",
       "4         0.124298      -0.351171      -0.216545       0.113899  \n",
       "...            ...            ...            ...            ...  \n",
       "9956     -0.191598      -0.111245      -0.011547       0.425088  \n",
       "9957     -0.197824      -0.099159      -0.018177       0.359986  \n",
       "9958     -0.181205      -0.077962      -0.024079       0.305890  \n",
       "9959     -0.219385       0.013357      -0.010018       0.192860  \n",
       "9960     -0.244424      -0.001849      -0.016634       0.224688  \n",
       "\n",
       "[9961 rows x 14 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>utterance</th>\n",
       "      <th>frame</th>\n",
       "      <th>coefficient1</th>\n",
       "      <th>coefficient2</th>\n",
       "      <th>coefficient3</th>\n",
       "      <th>coefficient4</th>\n",
       "      <th>coefficient5</th>\n",
       "      <th>coefficient6</th>\n",
       "      <th>coefficient7</th>\n",
       "      <th>coefficient8</th>\n",
       "      <th>coefficient9</th>\n",
       "      <th>coefficient10</th>\n",
       "      <th>coefficient11</th>\n",
       "      <th>coefficient12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.860936</td>\n",
       "      <td>-0.207383</td>\n",
       "      <td>0.261557</td>\n",
       "      <td>-0.214562</td>\n",
       "      <td>-0.171253</td>\n",
       "      <td>-0.118167</td>\n",
       "      <td>-0.277557</td>\n",
       "      <td>0.025668</td>\n",
       "      <td>0.126701</td>\n",
       "      <td>-0.306756</td>\n",
       "      <td>-0.213076</td>\n",
       "      <td>0.088728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.891651</td>\n",
       "      <td>-0.193249</td>\n",
       "      <td>0.235363</td>\n",
       "      <td>-0.249118</td>\n",
       "      <td>-0.112890</td>\n",
       "      <td>-0.112238</td>\n",
       "      <td>-0.311997</td>\n",
       "      <td>-0.027122</td>\n",
       "      <td>0.171457</td>\n",
       "      <td>-0.289431</td>\n",
       "      <td>-0.247722</td>\n",
       "      <td>0.093011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.939205</td>\n",
       "      <td>-0.239664</td>\n",
       "      <td>0.258561</td>\n",
       "      <td>-0.291458</td>\n",
       "      <td>-0.041053</td>\n",
       "      <td>-0.102034</td>\n",
       "      <td>-0.383300</td>\n",
       "      <td>0.019013</td>\n",
       "      <td>0.169510</td>\n",
       "      <td>-0.314894</td>\n",
       "      <td>-0.227908</td>\n",
       "      <td>0.074638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1.717517</td>\n",
       "      <td>-0.218572</td>\n",
       "      <td>0.217119</td>\n",
       "      <td>-0.228186</td>\n",
       "      <td>-0.018608</td>\n",
       "      <td>-0.137624</td>\n",
       "      <td>-0.403318</td>\n",
       "      <td>-0.009643</td>\n",
       "      <td>0.164607</td>\n",
       "      <td>-0.323267</td>\n",
       "      <td>-0.210105</td>\n",
       "      <td>0.098098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1.741191</td>\n",
       "      <td>-0.279891</td>\n",
       "      <td>0.196583</td>\n",
       "      <td>-0.236377</td>\n",
       "      <td>-0.032012</td>\n",
       "      <td>-0.090612</td>\n",
       "      <td>-0.363134</td>\n",
       "      <td>-0.012571</td>\n",
       "      <td>0.124298</td>\n",
       "      <td>-0.351171</td>\n",
       "      <td>-0.216545</td>\n",
       "      <td>0.113899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9956</th>\n",
       "      <td>29</td>\n",
       "      <td>7</td>\n",
       "      <td>1.216506</td>\n",
       "      <td>-0.424432</td>\n",
       "      <td>-0.034349</td>\n",
       "      <td>-0.294238</td>\n",
       "      <td>0.166270</td>\n",
       "      <td>-0.397758</td>\n",
       "      <td>0.091827</td>\n",
       "      <td>0.023178</td>\n",
       "      <td>-0.191598</td>\n",
       "      <td>-0.111245</td>\n",
       "      <td>-0.011547</td>\n",
       "      <td>0.425088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9957</th>\n",
       "      <td>29</td>\n",
       "      <td>8</td>\n",
       "      <td>1.214579</td>\n",
       "      <td>-0.399925</td>\n",
       "      <td>-0.127891</td>\n",
       "      <td>-0.239583</td>\n",
       "      <td>0.221423</td>\n",
       "      <td>-0.374714</td>\n",
       "      <td>0.046216</td>\n",
       "      <td>0.051754</td>\n",
       "      <td>-0.197824</td>\n",
       "      <td>-0.099159</td>\n",
       "      <td>-0.018177</td>\n",
       "      <td>0.359986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9958</th>\n",
       "      <td>29</td>\n",
       "      <td>9</td>\n",
       "      <td>1.170031</td>\n",
       "      <td>-0.306025</td>\n",
       "      <td>-0.145534</td>\n",
       "      <td>-0.195854</td>\n",
       "      <td>0.276371</td>\n",
       "      <td>-0.346794</td>\n",
       "      <td>0.042325</td>\n",
       "      <td>0.031668</td>\n",
       "      <td>-0.181205</td>\n",
       "      <td>-0.077962</td>\n",
       "      <td>-0.024079</td>\n",
       "      <td>0.305890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9959</th>\n",
       "      <td>29</td>\n",
       "      <td>10</td>\n",
       "      <td>1.118108</td>\n",
       "      <td>-0.258605</td>\n",
       "      <td>-0.103982</td>\n",
       "      <td>-0.231256</td>\n",
       "      <td>0.344866</td>\n",
       "      <td>-0.223807</td>\n",
       "      <td>-0.014415</td>\n",
       "      <td>-0.046215</td>\n",
       "      <td>-0.219385</td>\n",
       "      <td>0.013357</td>\n",
       "      <td>-0.010018</td>\n",
       "      <td>0.192860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9960</th>\n",
       "      <td>29</td>\n",
       "      <td>11</td>\n",
       "      <td>1.177449</td>\n",
       "      <td>-0.404080</td>\n",
       "      <td>0.052026</td>\n",
       "      <td>-0.284812</td>\n",
       "      <td>0.429840</td>\n",
       "      <td>-0.203484</td>\n",
       "      <td>-0.072393</td>\n",
       "      <td>-0.080955</td>\n",
       "      <td>-0.244424</td>\n",
       "      <td>-0.001849</td>\n",
       "      <td>-0.016634</td>\n",
       "      <td>0.224688</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9961 rows × 14 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T19:38:11.440871Z",
     "start_time": "2025-06-30T19:37:58.417335Z"
    }
   },
   "cell_type": "code",
   "source": "processNan(X_gcm, 1)",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     AFFX-BioB-5_at  AFFX-BioB-M_at  AFFX-BioB-3_at  AFFX-BioC-5_at  \\\n",
       "0             -73.0           -69.0           -48.0              13   \n",
       "1             -16.0           -63.0           -97.0             -42   \n",
       "2               4.0           -45.0          -112.0             -25   \n",
       "3             -31.0          -110.0           -20.0             -50   \n",
       "4             -33.0           -39.0           -45.0              14   \n",
       "..              ...             ...             ...             ...   \n",
       "185           -74.0          -125.0            31.0              50   \n",
       "186          -199.0          -566.0          -590.0             -85   \n",
       "187          -128.0          -264.0          -137.0             -57   \n",
       "188           -74.0         -3292.0         -2562.0            -330   \n",
       "189          -157.0          -225.0            43.0             163   \n",
       "\n",
       "     AFFX-BioC-3_at  AFFX-BioDn-5_at  AFFX-BioDn-3_at  AFFX-CreX-5_at  \\\n",
       "0             -86.0           -147.0              -65           -71.0   \n",
       "1             -91.0           -164.0              -53           -77.0   \n",
       "2             -85.0           -127.0               56          -110.0   \n",
       "3            -115.0           -113.0              -17           -40.0   \n",
       "4             -56.0           -106.0               73           -34.0   \n",
       "..              ...              ...              ...             ...   \n",
       "185          -133.0           -213.0             -184          -134.0   \n",
       "186          -606.0           -212.0             -391          -357.0   \n",
       "187          -455.0           -197.0              720          -227.0   \n",
       "188          -313.0           -178.0             -592           -61.0   \n",
       "189          -364.0           -348.0               39          -163.0   \n",
       "\n",
       "     AFFX-CreX-3_at  AFFX-BioB-5_st  ...  RC_D60524_f_at  RC_D60607_at  \\\n",
       "0             -32.0             100  ...          -134.0           352   \n",
       "1             -17.0             122  ...           -51.0           244   \n",
       "2              81.0              41  ...            14.0           163   \n",
       "3             -17.0              80  ...            26.0           625   \n",
       "4              18.0              64  ...           -69.0           398   \n",
       "..              ...             ...  ...             ...           ...   \n",
       "185           -53.0             -88  ...          -126.0          1462   \n",
       "186          -162.0             736  ...           938.0          2000   \n",
       "187           170.0             340  ...            30.0          1510   \n",
       "188           -58.0             -50  ...           -25.0           426   \n",
       "189           -85.0             340  ...           -38.0           547   \n",
       "\n",
       "     RC_D60608_at  RC_D60642_f_at  RC_D60670_at  RC_D60700_at  RC_D60715_at  \\\n",
       "0             -67             121          -5.0           -11         -21.0   \n",
       "1             -15             119         -32.0             4         -14.0   \n",
       "2             -14               7          15.0            -8        -104.0   \n",
       "3              18              59         -10.0            32          -2.0   \n",
       "4              38             215          -2.0            44           3.0   \n",
       "..            ...             ...           ...           ...           ...   \n",
       "185            35             271          62.0            83           6.0   \n",
       "186           681            4010         482.0          1092          24.0   \n",
       "187           331            2015         563.0           752        -103.0   \n",
       "188           231             253           8.0           -18        -121.0   \n",
       "189           305            1497         106.0           975         -24.0   \n",
       "\n",
       "     RC_D60774_f_at  RC_D60794_i_at  RC_D60794_f_at  \n",
       "0             -41.0          -967.0          -120.0  \n",
       "1             -28.0          -205.0           -31.0  \n",
       "2             -36.0          -245.0            34.0  \n",
       "3              10.0          -495.0           -37.0  \n",
       "4              68.0          -293.0           -34.0  \n",
       "..              ...             ...             ...  \n",
       "185            64.0         -1046.0           -59.0  \n",
       "186          1200.0         -2807.0             5.0  \n",
       "187           417.0         -2115.0          -108.0  \n",
       "188            56.0          -778.0          -102.0  \n",
       "189           757.0         -2054.0           -44.0  \n",
       "\n",
       "[190 rows x 16063 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AFFX-BioB-5_at</th>\n",
       "      <th>AFFX-BioB-M_at</th>\n",
       "      <th>AFFX-BioB-3_at</th>\n",
       "      <th>AFFX-BioC-5_at</th>\n",
       "      <th>AFFX-BioC-3_at</th>\n",
       "      <th>AFFX-BioDn-5_at</th>\n",
       "      <th>AFFX-BioDn-3_at</th>\n",
       "      <th>AFFX-CreX-5_at</th>\n",
       "      <th>AFFX-CreX-3_at</th>\n",
       "      <th>AFFX-BioB-5_st</th>\n",
       "      <th>...</th>\n",
       "      <th>RC_D60524_f_at</th>\n",
       "      <th>RC_D60607_at</th>\n",
       "      <th>RC_D60608_at</th>\n",
       "      <th>RC_D60642_f_at</th>\n",
       "      <th>RC_D60670_at</th>\n",
       "      <th>RC_D60700_at</th>\n",
       "      <th>RC_D60715_at</th>\n",
       "      <th>RC_D60774_f_at</th>\n",
       "      <th>RC_D60794_i_at</th>\n",
       "      <th>RC_D60794_f_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-73.0</td>\n",
       "      <td>-69.0</td>\n",
       "      <td>-48.0</td>\n",
       "      <td>13</td>\n",
       "      <td>-86.0</td>\n",
       "      <td>-147.0</td>\n",
       "      <td>-65</td>\n",
       "      <td>-71.0</td>\n",
       "      <td>-32.0</td>\n",
       "      <td>100</td>\n",
       "      <td>...</td>\n",
       "      <td>-134.0</td>\n",
       "      <td>352</td>\n",
       "      <td>-67</td>\n",
       "      <td>121</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-11</td>\n",
       "      <td>-21.0</td>\n",
       "      <td>-41.0</td>\n",
       "      <td>-967.0</td>\n",
       "      <td>-120.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-16.0</td>\n",
       "      <td>-63.0</td>\n",
       "      <td>-97.0</td>\n",
       "      <td>-42</td>\n",
       "      <td>-91.0</td>\n",
       "      <td>-164.0</td>\n",
       "      <td>-53</td>\n",
       "      <td>-77.0</td>\n",
       "      <td>-17.0</td>\n",
       "      <td>122</td>\n",
       "      <td>...</td>\n",
       "      <td>-51.0</td>\n",
       "      <td>244</td>\n",
       "      <td>-15</td>\n",
       "      <td>119</td>\n",
       "      <td>-32.0</td>\n",
       "      <td>4</td>\n",
       "      <td>-14.0</td>\n",
       "      <td>-28.0</td>\n",
       "      <td>-205.0</td>\n",
       "      <td>-31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>-45.0</td>\n",
       "      <td>-112.0</td>\n",
       "      <td>-25</td>\n",
       "      <td>-85.0</td>\n",
       "      <td>-127.0</td>\n",
       "      <td>56</td>\n",
       "      <td>-110.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>41</td>\n",
       "      <td>...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>163</td>\n",
       "      <td>-14</td>\n",
       "      <td>7</td>\n",
       "      <td>15.0</td>\n",
       "      <td>-8</td>\n",
       "      <td>-104.0</td>\n",
       "      <td>-36.0</td>\n",
       "      <td>-245.0</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-31.0</td>\n",
       "      <td>-110.0</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>-50</td>\n",
       "      <td>-115.0</td>\n",
       "      <td>-113.0</td>\n",
       "      <td>-17</td>\n",
       "      <td>-40.0</td>\n",
       "      <td>-17.0</td>\n",
       "      <td>80</td>\n",
       "      <td>...</td>\n",
       "      <td>26.0</td>\n",
       "      <td>625</td>\n",
       "      <td>18</td>\n",
       "      <td>59</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>32</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-495.0</td>\n",
       "      <td>-37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-33.0</td>\n",
       "      <td>-39.0</td>\n",
       "      <td>-45.0</td>\n",
       "      <td>14</td>\n",
       "      <td>-56.0</td>\n",
       "      <td>-106.0</td>\n",
       "      <td>73</td>\n",
       "      <td>-34.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>64</td>\n",
       "      <td>...</td>\n",
       "      <td>-69.0</td>\n",
       "      <td>398</td>\n",
       "      <td>38</td>\n",
       "      <td>215</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>44</td>\n",
       "      <td>3.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>-293.0</td>\n",
       "      <td>-34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>-74.0</td>\n",
       "      <td>-125.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>50</td>\n",
       "      <td>-133.0</td>\n",
       "      <td>-213.0</td>\n",
       "      <td>-184</td>\n",
       "      <td>-134.0</td>\n",
       "      <td>-53.0</td>\n",
       "      <td>-88</td>\n",
       "      <td>...</td>\n",
       "      <td>-126.0</td>\n",
       "      <td>1462</td>\n",
       "      <td>35</td>\n",
       "      <td>271</td>\n",
       "      <td>62.0</td>\n",
       "      <td>83</td>\n",
       "      <td>6.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>-1046.0</td>\n",
       "      <td>-59.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>-199.0</td>\n",
       "      <td>-566.0</td>\n",
       "      <td>-590.0</td>\n",
       "      <td>-85</td>\n",
       "      <td>-606.0</td>\n",
       "      <td>-212.0</td>\n",
       "      <td>-391</td>\n",
       "      <td>-357.0</td>\n",
       "      <td>-162.0</td>\n",
       "      <td>736</td>\n",
       "      <td>...</td>\n",
       "      <td>938.0</td>\n",
       "      <td>2000</td>\n",
       "      <td>681</td>\n",
       "      <td>4010</td>\n",
       "      <td>482.0</td>\n",
       "      <td>1092</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>-2807.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>-128.0</td>\n",
       "      <td>-264.0</td>\n",
       "      <td>-137.0</td>\n",
       "      <td>-57</td>\n",
       "      <td>-455.0</td>\n",
       "      <td>-197.0</td>\n",
       "      <td>720</td>\n",
       "      <td>-227.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>340</td>\n",
       "      <td>...</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1510</td>\n",
       "      <td>331</td>\n",
       "      <td>2015</td>\n",
       "      <td>563.0</td>\n",
       "      <td>752</td>\n",
       "      <td>-103.0</td>\n",
       "      <td>417.0</td>\n",
       "      <td>-2115.0</td>\n",
       "      <td>-108.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>-74.0</td>\n",
       "      <td>-3292.0</td>\n",
       "      <td>-2562.0</td>\n",
       "      <td>-330</td>\n",
       "      <td>-313.0</td>\n",
       "      <td>-178.0</td>\n",
       "      <td>-592</td>\n",
       "      <td>-61.0</td>\n",
       "      <td>-58.0</td>\n",
       "      <td>-50</td>\n",
       "      <td>...</td>\n",
       "      <td>-25.0</td>\n",
       "      <td>426</td>\n",
       "      <td>231</td>\n",
       "      <td>253</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-18</td>\n",
       "      <td>-121.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>-778.0</td>\n",
       "      <td>-102.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>-157.0</td>\n",
       "      <td>-225.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>163</td>\n",
       "      <td>-364.0</td>\n",
       "      <td>-348.0</td>\n",
       "      <td>39</td>\n",
       "      <td>-163.0</td>\n",
       "      <td>-85.0</td>\n",
       "      <td>340</td>\n",
       "      <td>...</td>\n",
       "      <td>-38.0</td>\n",
       "      <td>547</td>\n",
       "      <td>305</td>\n",
       "      <td>1497</td>\n",
       "      <td>106.0</td>\n",
       "      <td>975</td>\n",
       "      <td>-24.0</td>\n",
       "      <td>757.0</td>\n",
       "      <td>-2054.0</td>\n",
       "      <td>-44.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>190 rows × 16063 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T19:38:16.555007Z",
     "start_time": "2025-06-30T19:38:16.542005Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = \"darkgreen\">4- Traitement des caractéristiques</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Avant de faire appel à un modèle d'apprentissage automatique sur un jeu de données, l'étude de corrélation entre caractéristiques pourrait être menée. Si un couple de caractéristiques ayant une très forte corrélation alors l'une des deux caractéristiques est supprimée.\n",
    "\n",
    "D'après vous, pourquoi on effectue cette suppression?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T19:38:17.156494Z",
     "start_time": "2025-06-30T19:38:17.145495Z"
    }
   },
   "source": "# pour éviter d'avoir les memes information, si deux caractéristiques sont fortement corrélées alors elles apportent la même information. ça peut affecter la performance du modèle d'apprentissage automatique.",
   "outputs": [],
   "execution_count": 42
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " L'un des moyens pour mesurer la corrélation entre deux caractéristiques est l'utilisation de la mesure de Spearman. cette dernière mesure la force et la direction de la relation n liant les deux caractéristiques.  En ce qui concerne la force de la relation, la valeur du coefficient de corrélation renvoyé par Spearman varie entre +1 et -1.  Une valeur de ± 1 indique un degré de relation parfait entre les deux variables.  Plus la valeur du coefficient de corrélation se rapproche de 0, plus la relation entre les deux variables sera faible.  La direction de la relation est indiquée par le signe du coefficient ; un signe + indique une relation positive et un signe - une relation négative. Si le coefficient est équivant à +1 (repectivement -1) alors il signifie que plus les valeurs d'une caractéristique est grande plus les valeurs de l'autre caractéristique est grande (respectivement petite). \n",
    "\n",
    " Développez une fonction qui retourne, pour une caractéristique donnée, les indices de toutes les caractéristiques dont le coefficient de corrélation retourné par la mesure de Spearman dépasse un certain seuil qu'on appelera t (entré en paramètre). \n",
    "\n",
    "\n",
    "#### Notes: \n",
    "Vous pourriez utiliser la mesure de Spearman proposée par Scipy(https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.spearmanr.html). Attention, nous nous intéressons seulement au coefficient de corrélation retournée dans la variable corrélation.  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T19:38:18.547179Z",
     "start_time": "2025-06-30T19:38:17.348681Z"
    }
   },
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "\n",
    "def calcule_correlation(X, index, t):\n",
    "    correlated_indices = []\n",
    "    for i in range(X.shape[1]):\n",
    "        if i != index:\n",
    "            corr, _ = spearmanr(X.iloc[:, index], X.iloc[:, i])\n",
    "            if abs(corr) > t:\n",
    "                correlated_indices.append(i)\n",
    "\n",
    "    return correlated_indices"
   ],
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T19:38:18.813186Z",
     "start_time": "2025-06-30T19:38:18.662183Z"
    }
   },
   "cell_type": "code",
   "source": "calcule_correlation(X_diabetes, 0, 0.6)",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T19:38:19.015179Z",
     "start_time": "2025-06-30T19:38:18.940181Z"
    }
   },
   "cell_type": "code",
   "source": "calcule_correlation(X_japanese, 0, 0.6)",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T19:38:25.123107Z",
     "start_time": "2025-06-30T19:38:24.504081Z"
    }
   },
   "cell_type": "code",
   "source": "calcule_correlation(X_wrn, 0, 0.6)",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[22, 23]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 46
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Développez une fonction qui supprime d'un jeu de données des caractéristiques à partir de leur indice renseignés en paramètres sous forme de liste. Le but ici est de supprimer les caractéistiques non nécessaires.\n",
    " \n",
    " \n",
    " On peut, supprimer si on le souhaite, les caractériqtiques à partir des noms de colonnes au lieu des indices de colonnes."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T19:38:25.326114Z",
     "start_time": "2025-06-30T19:38:25.315108Z"
    }
   },
   "source": [
    "def remove_columns(X, indices):\n",
    "    return X.drop(X.columns[indices], axis=1)"
   ],
   "outputs": [],
   "execution_count": 47
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Développez une fonction appelée processCor qui :\n",
    "- parcourt les caractéristiques \n",
    "- et supprime à chaque fois les caractéristiques qui ont \n",
    "un coefficient de corrélation avec la caractéristique courante dépassant le seuil t. \n",
    "\n",
    "Utilisez pour cela les deux fonctions developpées précedement dans la partie 4."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T19:38:25.451112Z",
     "start_time": "2025-06-30T19:38:25.440125Z"
    }
   },
   "source": [
    "def processCor(X, t):\n",
    "    i = 0\n",
    "    while i < X.shape[1]:\n",
    "        correlated_indices = calcule_correlation(X, i, t)\n",
    "        if i in correlated_indices:\n",
    "            correlated_indices.remove(i)\n",
    "        if correlated_indices:\n",
    "            X = remove_columns(X, correlated_indices)\n",
    "        else:\n",
    "            i += 1\n",
    "    return X\n"
   ],
   "outputs": [],
   "execution_count": 48
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = \"darkgreen\">5- Clustering </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Dans cette partie, nous allons associer à un problème (jeu de données) le meilleur modèle de clustering. En conséquence, nous considérons que les jeux de données n'ont pas de labels ou de colonne classe, autrement dit on s'intéresse seulement aux caractéristiques (features). Dans ce cadre, on se focalisera que sur les méthodes de clustering :\n",
    "- Dbscan(https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html#sklearn.cluster.DBSCAN)\n",
    "- Clustering agglomératif(https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering)\n",
    "- K-means\n",
    "#### Notes:\n",
    "* Un modèle est différent d'un algorithme(méthode). Dans le contexte d'apprentissage automatique,  un algorithme est une procédure exécutée sur un jeu de données. Le modèle est le résultat produit par l'agorithme. \n",
    "* Pour fournir un résultat (modèle), l'algorithme pourrait se baser sur un ensemble d'hyperparamètres qui lui sont fournis en entrée. Un hyperparamètre est une variable qui prend une valeur parmi un ensemble ou une infinité de valeurs, son but est de contrôler le processus d'apprentissage automatique (par exemple, le clustering ou la classification supervisée). \n",
    "* Différentes valeurs des hyperparamètres pourraient aboutir à différents modèles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Créez une collection de jeux de données appelée original_datasets. La clé correspond au nom du jeu de données et la valeur correspondante est un tuple (X,y) où X est l'ensemble des caractéristiques et y l'ensemble des labels.  \n",
    "#### Note:\n",
    "Utilisez la fonction développée dans la partie 2."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T19:38:25.668112Z",
     "start_time": "2025-06-30T19:38:25.563106Z"
    }
   },
   "source": [
    "original_datasets = {\n",
    "    \"diabetes\": (split_data(\"dataset/data/diabetes.csv\",\"Outcome\")),\n",
    "    \"japanese\": (split_data(\"dataset/data/JapaneseVowels.csv\",\"speaker\")),\n",
    "    \"wrn\": (split_data(\"dataset/data/wall-robot-navigation.csv\",\"Class\")),\n",
    "    #\"gcm\": (split_data(\"dataset/data/GCM.csv\", \"class\"))\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 49
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons préparer les jeux de données avant leur consommation par les méthodes de clustering. Tout d'abord créez une copie de  original_datasets qu'on appelera datasets. Ensuite traitez chaque jeu de données de datasets avec les fonctions processNan et processCor. On fixera typeOp à 1 et t à 0.7."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T19:38:25.825106Z",
     "start_time": "2025-06-30T19:38:25.814111Z"
    }
   },
   "source": "dataset = original_datasets.copy()",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T19:38:28.760031Z",
     "start_time": "2025-06-30T19:38:25.952106Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for key, (X, y) in dataset.items():\n",
    "    X = processNan(X, 1)  # Remplacer les valeurs nulles par la moyenne\n",
    "    X = processCor(X, 0.7)  # Supprimer les caractéristiques corrélées\n",
    "    dataset[key] = (X, y)  # Mettre à jour le jeu de données traité"
   ],
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T19:38:28.979035Z",
     "start_time": "2025-06-30T19:38:28.968042Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En utilisant la collection datasets, appelez les trois méthodes de clustering sur chaque jeu de données de la collection datasets et évaluez le résultat de  partitionnement(clustering) via:\n",
    "* Calinski Harabaz(https://scikit-learn.org/stable/modules/generated/sklearn.metrics.calinski_harabasz_score.html#sklearn.metrics.calinski_harabasz_score)\n",
    "* Davies Bouldin(https://scikit-learn.org/stable/modules/generated/sklearn.metrics.davies_bouldin_score.html#sklearn.metrics.davies_bouldin_sc)\n",
    "\n",
    "#### Notes:\n",
    "* Sachant qu'on considère que les labels ne sont pas fournis, nous évaluons les résultats aussi par des métriques d'évaluation non supervisées, c'est à dire que les métriques n'ont pas accès aux labels pour proposer un score de performance.\n",
    "* On considère que meilleur est le score des métriques d'évaluation mieux le problème associé au jeu de données est résolu.\n",
    "* Lors de l'appel des méthodes de clustering, dans cette partie on modifiera pas les valeurs des hyperparamètres, on se contentera des valeurs fixés par défaut par scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Associez à chaque paire \"jeu de données et méthode de clustering\" les scores retournés par les métriques d'évaluation (calinski, davies)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T19:38:29.820851Z",
     "start_time": "2025-06-30T19:38:29.170040Z"
    }
   },
   "source": [
    "from sklearn.cluster import DBSCAN, AgglomerativeClustering, KMeans\n",
    "from sklearn.metrics import calinski_harabasz_score, davies_bouldin_score\n"
   ],
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T19:38:30.008364Z",
     "start_time": "2025-06-30T19:38:29.997370Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_clustering(X, method_name):\n",
    "    if method_name == \"dbscan\":\n",
    "        model = DBSCAN()\n",
    "    elif method_name == \"agglomerative\":\n",
    "        model = AgglomerativeClustering()\n",
    "    elif method_name == \"kmeans\":\n",
    "        model = KMeans()\n",
    "    else:\n",
    "        raise ValueError(\"Méthode inconnue\")\n",
    "\n",
    "    labels = model.fit_predict(X)\n",
    "\n",
    "    if len(set(labels)) > 1:\n",
    "        calinski_score = calinski_harabasz_score(X, labels)\n",
    "        davies_score = davies_bouldin_score(X, labels)\n",
    "    else:\n",
    "        calinski_score = float('nan')\n",
    "        davies_score = float('nan')\n",
    "\n",
    "    return calinski_score, davies_score"
   ],
   "outputs": [],
   "execution_count": 53
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour chaque jeu de données, quelle est la méthode qui a fourni le meilleur modèle selon les scores calculés dans la question précedente."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T19:38:41.303086Z",
     "start_time": "2025-06-30T19:38:30.152365Z"
    }
   },
   "source": [
    "for key, (X, y) in dataset.items():\n",
    "    print(f\"Jeu de données: {key}\")\n",
    "    for method in [\"dbscan\", \"agglomerative\", \"kmeans\"]:\n",
    "        calinski_score, davies_score = evaluate_clustering(X, method)\n",
    "        print(f\"  Méthode: {method}, Calinski: {calinski_score}, Davies: {davies_score}\")\n",
    "    print()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jeu de données: diabetes\n",
      "  Méthode: dbscan, Calinski: nan, Davies: nan\n",
      "  Méthode: agglomerative, Calinski: 907.291128158236, Davies: 0.7444321463457898\n",
      "  Méthode: kmeans, Calinski: 1005.6877169279542, Davies: 0.8487726330797032\n",
      "\n",
      "Jeu de données: japanese\n",
      "  Méthode: dbscan, Calinski: nan, Davies: nan\n",
      "  Méthode: agglomerative, Calinski: 7632.094168843869, Davies: 0.35910087331353424\n",
      "  Méthode: kmeans, Calinski: 14629.977917937767, Davies: 0.8204514755763403\n",
      "\n",
      "Jeu de données: wrn\n",
      "  Méthode: dbscan, Calinski: 9.771660887017699, Davies: 1.1178167891986839\n",
      "  Méthode: agglomerative, Calinski: 840.8065337810509, Davies: 2.209461083507007\n",
      "  Méthode: kmeans, Calinski: 579.9329357388772, Davies: 1.880541451421756\n",
      "\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* Calinski score:\n",
    "    * Score élevé = bon clustering\n",
    "    * Les clusters sont bien séparés et compacts.\n",
    "    * Score faible = mauvais clustering\n",
    "    * Les clusters sont trop proches ou trop étalés.\n",
    "* Davies score:\n",
    "    * Score faible = bon clustering\n",
    "    * Les clusters sont bien séparés et compacts.\n",
    "    * Score élevé = mauvais clustering\n",
    "    * Les clusters sont trop proches ou trop étalés."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* Pour Diabetes le meilleurs modeles est Agglomerative\n",
    "* Pour Japanese le meilleurs modeles est K-means\n",
    "* Pour WRN le meilleurs modeles est Agglomerative (différencie bien les types de mouvements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = \"darkgreen\">6- Peut-on améliorer encore les résultats de clustering? </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette partie nous nous intéressons toujours au clustering mais das une perspective d'amélioration des modèles de clustering produits précedement. L'une des pistes d'amélioration des modèles est de varier les valeurs des hyper-paramètres des méthodes de clustering ainsi que les types de pré-traitement appliqués aux jeux de données. \n",
    "\n",
    "Tout d'abord développez une fonction (qu'on appellera getBestModel) qui pour une méthode de clustering donnée et un jeu de données renvoie le meilleur modèle. La fonction prend en paramètre le jeu de données X, le nom de la méthode methodName et une collection d'hyper-paramètres params. Cette collection a pour clé le nom de l'hyper-paramètre et pour valeur la liste des valeurs prises par l'hyper-paramètre. Dans cette fonction, toutes les combinaisons des valeurs des listes dans params seront évaluées et à chaque combinaison on obtiendra un modèle associée au jeu de données et à la méthode de clustering renseignés en paramètre. Chaque modèle est évalué par une métrique d'évaluation au choix (calinski ou davies). \n",
    "\n",
    "La collection params comporte les valeurs possibles des hyper-paramètres de la méthode clustering concernée ainsi que les valeurs possibles de typeOp et t. Vous avez le choix de fixer la liste de valeurs de typeOp et t. Vous pourriez commencer par évaluer [1,2,3,4] pour typeOp et [0.5, 0.7, 0.9] pour t. Les valeurs des hyper-paramètres dépendent de la méthode de clustering ciblée:\n",
    "\n",
    "* Dbscan a plusieurs hyper-paramètres dont les plus influents sont eps, min_samples et metric. Notons que eps est un réel et ne peut dépasser la distance inter-points maximale qui existe dans X, et min_samples est un entier naturel non nul inférieur au nombre d'observations dans X. Metric a par défaut la valeur euclidean, d'autres valeurs sont possibles et consultables sur https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html#sklearn.metrics.pairwise_distances. Vous avez le choix de fixer les listes de ces trois hyper-paramètres. On vous suggère de ne inclure des valeurs dépassant $\\sqrt{|X|}$ pour min_samples et la moyenne des distances inter-points pour eps.\n",
    "* Clustering agglomératif a pour hyper-paramètres les plus sensibles n_clusters, distance_threshold, affinity(équivalent de metric dans Dbscan) et linkage. A savoir que soit n_clusters ou distance_threshold est utilisé pour extraire les clusters finaux, il n'est pas nécessaire d'utiliser les deux. Vous pourriez limiter la valeur de n_clusters à $\\frac{\\sqrt{|X|}}{2}$ ou limiter la valeur de distance_threshold à la moyenne des distances inter-points.\n",
    "* K-means a deux hyper-paramètres sensibles: n_clusters(nombre de clusters) et l'initialisation des centroides init{‘k-means++’, ‘random’}. \n",
    "\n",
    "#### Note:\n",
    "Vous aurez à instancier plusieurs fois la collection datasets en fonction des valeurs prises par typeOp et t.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T19:38:47.648195Z",
     "start_time": "2025-06-30T19:38:47.620198Z"
    }
   },
   "source": [
    "from sklearn.metrics import calinski_harabasz_score, davies_bouldin_score\n",
    "\n",
    "def getBestModel(X, methodName, params):\n",
    "    bestScore = float('inf')\n",
    "    bestModel = None\n",
    "    bestParams = None\n",
    "\n",
    "    # Construire toutes les combinaisons possibles de paramètres\n",
    "    import itertools\n",
    "    keys, values = zip(*params.items())\n",
    "    for v in itertools.product(*values):\n",
    "        param = dict(zip(keys, v))\n",
    "        # Instanciation modèle selon méthode\n",
    "        if methodName == 'DBSCAN':\n",
    "            model = DBSCAN(**param)\n",
    "        elif methodName == 'KMeans':\n",
    "            model = KMeans(**param, random_state=0)\n",
    "        elif methodName == 'Agglomerative':\n",
    "            model = AgglomerativeClustering(**param)\n",
    "        else:\n",
    "            raise ValueError(\"Méthode inconnue\")\n",
    "\n",
    "        labels = model.fit_predict(X)\n",
    "        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "        # Sauter si nombre de clusters invalide pour les scores\n",
    "        if n_clusters < 2 or n_clusters >= len(X):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            calinski = calinski_harabasz_score(X, labels)\n",
    "            davies = davies_bouldin_score(X, labels)\n",
    "        except Exception as e:\n",
    "            # Ignorer paramètres qui provoquent des erreurs\n",
    "            continue\n",
    "\n",
    "        if davies < bestScore:\n",
    "            bestScore = davies\n",
    "            bestModel = model\n",
    "            bestParams = param\n",
    "\n",
    "    return bestModel, bestParams, bestScore\n"
   ],
   "outputs": [],
   "execution_count": 55
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En utilisant la fonction getBestModel, identifiez le meilleur modèle associé à chaque jeu de données et méthode de clustering."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T19:40:50.118282Z",
     "start_time": "2025-06-30T19:38:47.901194Z"
    }
   },
   "source": [
    "for key, (X, y) in dataset.items():\n",
    "    print(f\"Jeu de données: {key}\")\n",
    "\n",
    "    params_dbscan = {\n",
    "        'eps': [0.5, 0.7, 0.9],\n",
    "        'min_samples': [1, 2, 3],\n",
    "        'metric': ['euclidean', 'manhattan']\n",
    "    }\n",
    "    params_kmeans = {\n",
    "        'n_clusters': [2, 3, 4],\n",
    "        'init': ['k-means++', 'random']\n",
    "    }\n",
    "    params_agglo = {\n",
    "        'n_clusters': [2, 3, 4],\n",
    "        'linkage': ['ward', 'complete', 'average']\n",
    "    }\n",
    "\n",
    "    for method in [\"DBSCAN\", \"KMeans\", \"Agglomerative\"]:\n",
    "        if method == \"DBSCAN\":\n",
    "            params = params_dbscan\n",
    "        elif method == \"KMeans\":\n",
    "            params = params_kmeans\n",
    "        else:\n",
    "            params = params_agglo\n",
    "\n",
    "        best_model, best_params, best_score = getBestModel(X, method, params)\n",
    "        print(f\"  Méthode: {method}\")\n",
    "        print(f\"    Meilleurs paramètres: {best_params}\")\n",
    "        print(f\"    Meilleur score Davies-Bouldin: {best_score:.2f}\")\n",
    "\n",
    "    print()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jeu de données: diabetes\n",
      "  Méthode: DBSCAN\n",
      "    Meilleurs paramètres: None\n",
      "    Meilleur score Davies-Bouldin: inf\n",
      "  Méthode: KMeans\n",
      "    Meilleurs paramètres: {'n_clusters': 3, 'init': 'k-means++'}\n",
      "    Meilleur score Davies-Bouldin: 0.67\n",
      "  Méthode: Agglomerative\n",
      "    Meilleurs paramètres: {'n_clusters': 2, 'linkage': 'average'}\n",
      "    Meilleur score Davies-Bouldin: 0.24\n",
      "\n",
      "Jeu de données: japanese\n",
      "  Méthode: DBSCAN\n",
      "    Meilleurs paramètres: {'eps': 0.5, 'min_samples': 1, 'metric': 'manhattan'}\n",
      "    Meilleur score Davies-Bouldin: 0.02\n",
      "  Méthode: KMeans\n",
      "    Meilleurs paramètres: {'n_clusters': 2, 'init': 'k-means++'}\n",
      "    Meilleur score Davies-Bouldin: 0.69\n",
      "  Méthode: Agglomerative\n",
      "    Meilleurs paramètres: {'n_clusters': 2, 'linkage': 'ward'}\n",
      "    Meilleur score Davies-Bouldin: 0.36\n",
      "\n",
      "Jeu de données: wrn\n",
      "  Méthode: DBSCAN\n",
      "    Meilleurs paramètres: {'eps': 0.5, 'min_samples': 1, 'metric': 'manhattan'}\n",
      "    Meilleur score Davies-Bouldin: 0.11\n",
      "  Méthode: KMeans\n",
      "    Meilleurs paramètres: {'n_clusters': 4, 'init': 'random'}\n",
      "    Meilleur score Davies-Bouldin: 2.10\n",
      "  Méthode: Agglomerative\n",
      "    Meilleurs paramètres: {'n_clusters': 2, 'linkage': 'average'}\n",
      "    Meilleur score Davies-Bouldin: 1.23\n",
      "\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par rapport à la partie 5, y a t-il eu des améliorations? Justifiez."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T19:40:50.460313Z",
     "start_time": "2025-06-30T19:40:50.446283Z"
    }
   },
   "source": [
    "#oui , on peut jouer avec les parametres pour améliorer les résultats. Par exemple, on peut changer la valeur de eps dans DBSCAN ou le nombre de clusters dans KMeans. On peut aussi changer le type de distance utilisée dans les méthodes de clustering.\n",
    "#on a les meilleurs paramètres pour chaque méthode de clustering et chaque jeu de données. On peut donc dire que les résultats sont meilleurs que ceux obtenus dans la partie 5 vec les parametre par defaut.\n",
    "#Si on entraine notre modele dans la partie 5 avec les parametre avec les meilleurs parametres obtenus dans la partie 6, on aura de meilleurs résultats."
   ],
   "outputs": [],
   "execution_count": 57
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = \"darkgreen\">7- La classification supervisée</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette partie nous chercherons à résoudre les problèmes par la classification supervisée. Dans ce cas, on considère que les labels (y) sont à notre disposition. Nous nous intéressons à trois méthodes de cette catégorie:\n",
    "* arbre de décision (https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier)\n",
    "* KNN : https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T19:49:49.273052Z",
     "start_time": "2025-06-30T19:49:49.252010Z"
    }
   },
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score"
   ],
   "outputs": [],
   "execution_count": 60
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instancier la collection datasets dans les mêmes conditions que celles spécifiées dans la partie 5."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T19:49:56.970143Z",
     "start_time": "2025-06-30T19:49:54.418170Z"
    }
   },
   "source": [
    "dataset = original_datasets.copy()\n",
    "for key, (X, y) in dataset.items():\n",
    "    X = processNan(X, 1)  # Remplacer les valeurs nulles par la moyenne\n",
    "    X = processCor(X, 0.7)  # Supprimer les caractéristiques corrélées\n",
    "    dataset[key] = (X, y)  # Mettre à jour le jeu de données traité"
   ],
   "outputs": [],
   "execution_count": 61
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sur chaque jeu de données de la collection datasets appliquez les deux méthodes de classification. A chaque application, évaluez le résultat (modèle) via les métriques d'évaluation suivantes:\n",
    "* accuracy score (https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score)\n",
    "* balanced accuracy score (https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html#sklearn.metrics.balanced_accuracy_score)\n",
    "\n",
    "#### Notes.\n",
    "* A l'appel des méthodes de classification, les valeurs des hyper-paramètres doivent restés fixés tel qu'elles sont par scikit-learn. \n",
    "* Avant d'appliquer les trois méthodes de classification sur le jeu de données, celui-ci doit être divisé en deux sous-ensembles: ensemble d'entraînement ($~80$% du jeu de données) et l'ensemble de test ($~20$%)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T19:50:00.732176Z",
     "start_time": "2025-06-30T19:49:59.931180Z"
    }
   },
   "source": [
    "\n",
    "for key, (X, y) in dataset.items():\n",
    "    print(key)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Modèle d'arbre de décision\n",
    "    model_dt = DecisionTreeClassifier()\n",
    "    model_dt.fit(X_train, y_train)\n",
    "    y_pred_dt = model_dt.predict(X_test)\n",
    "\n",
    "    # Modèle d'arbre de décision\n",
    "    model_knn = KNeighborsClassifier()\n",
    "    model_knn.fit(X_train, y_train)\n",
    "    y_pred_knn = model_knn.predict(X_test)\n",
    "\n",
    "    # Calcul des métriques\n",
    "    accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
    "    balanced_accuracy_dt = balanced_accuracy_score(y_test, y_pred_dt)\n",
    "\n",
    "    # Calcul des métriques\n",
    "    accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
    "    balanced_accuracy_knn = balanced_accuracy_score(y_test, y_pred_knn)\n",
    "\n",
    "    print(f\"  Arbre de décision - Accuracy: {accuracy_dt:.2f}, Balanced Accuracy: {balanced_accuracy_dt:.2f}\")\n",
    "    print(f\"  KNN - Accuracy: {accuracy_knn:.2f}, Balanced Accuracy: {balanced_accuracy_knn:.2f}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diabetes\n",
      "  Arbre de décision - Accuracy: 0.64, Balanced Accuracy: 0.62\n",
      "  KNN - Accuracy: 0.60, Balanced Accuracy: 0.57\n",
      "japanese\n",
      "  Arbre de décision - Accuracy: 0.86, Balanced Accuracy: 0.86\n",
      "  KNN - Accuracy: 0.46, Balanced Accuracy: 0.44\n",
      "wrn\n",
      "  Arbre de décision - Accuracy: 0.98, Balanced Accuracy: 0.98\n",
      "  KNN - Accuracy: 0.84, Balanced Accuracy: 0.83\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelle est la méthode ayant a fourni le meilleur modèle (c'est à dire le meilleur taux de prédiction) pour chaque jeu de données?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T19:40:53.155418Z",
     "start_time": "2025-06-30T19:40:53.143357Z"
    }
   },
   "source": "# Pour les 3 datasets la méthode qui a fourni le meilleur model c'est Accuracy",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = \"darkgreen\">8- Peut-on encore améliorer les résultats de classification supervisée</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans la même perspective que dans la partie 6, nous chercherons ici à améliorer les résultats de la classification en faisant varier les valeurs des hyper-paramètres et les types de pré-traitement des données. \n",
    "\n",
    "Reprenez la méthode gestBestModel en faisant en copie de celle-ci et en la nommant getBestModelC. Cette dernière a la même fonction que gestBestModel sauf qu'elle est adaptée pour la classification supervisée: \n",
    "* la métrique d'évaluation doit correspondre à l'une des deux métriques de la partie 7. \n",
    "* Pour faire face aux problèmes de désiquilibre de répartition de classes de points dans les sous-ensembles d'entraînement et de test, on adoptera la technique d'échantillonage suivante. A partir de chaque jeu de données, on génére 5 sous-ensembles de données disjoints. A chaque itération, un sous-ensemble est utilisé pour le test et les autres pour la phase d'entraînement. Un sous-ensemble doit être utilisé une seule fois dans la phase de test. \n",
    "\n",
    "Les listes de valeurs de typeOp et t peuvent être fixées identiquement à celles de la partie 6. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T19:40:53.295356Z",
     "start_time": "2025-06-30T19:40:53.281380Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En utilisant la fonction getBestModelC, identifiez le meilleur modèle associé à chaque jeu de données et méthode de classification?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T19:40:53.436393Z",
     "start_time": "2025-06-30T19:40:53.423359Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par rapport à la partie 7, y a t-il eu des améliorations? Justifiez."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T19:40:53.562392Z",
     "start_time": "2025-06-30T19:40:53.549358Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
